# Python Learning with Python. Second Edition

[toc]

## 1 什么是深度学习

### 1.1. 人工智能、机器学习和深度学习

$$深度学习\in 机器学习\in 人工智能$$

#### 1.1.1 人工智能

虽然许多基本理念在数年前就已经开始酝酿，但“人工智能”最终在1956年明确称为一个研究领域。当时达特茅斯学院年轻的数学系助理教授John McCarthy根据以下提案组织了一场夏季研讨会。

>该研究是基于以下猜想进行的：学习的各个方面或其他任何智能特征原则上都可以被精确描述，从而可以制造一台机器来模拟。我们将试图找到一种方法，让机器能够使用语言、形成抽象思维和概念、解决人类目前还不能解决的各种问题，并自我提升。我们认为，如果一组优秀的科学家在一起工作一个夏天，那么可以在其中一个或多个问题上取得重大进展。

夏天过去了，研讨会在结束时并没有完全解开它一开始打算研究的谜题。然而，许多参会者后来成为了这一领域的先驱，这次研讨也启动了一场延续至今的知识革命。

简而言之，人工智能可以被描述为试图将通常由人类完成的智力任务自动化。因此，人工智能是一个综合领域，不仅包括机器学习和深度学习，还包括更多不涉及学习的方法。直到20世纪80年代，大多数人工智能教科书中根本没有出现过“学习”二字。举个例子，早期的国际象棋程序仅涉及程序员手动编写的硬编码规则，，不能算作机器学习。事实上，在相当长的时间内，大多数专家相信，只要程序员手动编写足够多的明确规则来处理存储在显式数据库中的知识，就可以实现与人类水平相当的人工智能。这一方法被称为**符号主义人工智能**，从20世纪50年代到80年代末，它是人工智能的主流范式。在20世纪80年代的专家系统热潮中，这一方法的热度达到顶峰。

虽然符号主义人工智能适合用来解决定义明确的逻辑问题，比如下国际象棋，但它难以给出明确规则来解决更复杂、更模糊的问题，比如图像分类、语音识别或自然语言翻译。于是，一种替代符号主义人工智能的新方法出现了，这就是机器学习。

#### 1.1.2 机器学习

让计算机有效工作的常用方法是，由人类程序员编写规则，计算机遵循这些规则将输入数据转换为适当的答案。机器学习把这个过程反了过来：机器读取输入数据和相应的答案，然后找出应有的规则。机器学习系统是训练出来的，而不是明确地用程序写出来的。将与某些任务相关的许多示例输入机器学习系统，它会在这些示例中找到统计结构，从而最终找到将任务自动化的规则。

虽然在20世纪90年代才开始蓬勃发展，但机器学习已经循序成为人工智能最受欢迎且最成功的分支领域。这一发展趋势的驱动力来自速度更快的硬件与更大的数据集。机器学习与数理统计相关，但二者在在几个重要方面有所不同。与统计学相比，机器学习经常要处理复杂的大型数据集（比如包含数百万张图片的数据集，每张图片又包含数万像素），用经典的统计分析（比如贝叶斯分析）来处理这种数据集是不切实际的。因此，机器学习（尤其是深度学习）呈现出相对较少的数学理论（可能过于少了），从根本上来说是一门工程学科。

#### 1.1.3 从数据中学习规则与表示

为了给出深度学习的定义并搞清楚深度学习与其他机器学习方法的区别，我们首先需要了解机器学习算法在做什么。对于一项数据处理任务，给定预期输出的示例，机器学习系统可以发现执行任务的规则。即，我们需要已下3个要素来进行机器学习。

- 输入数据。
- 预期输出的示例
- 衡量算法效果的方法。这一衡量方法很有必要，其目的是计算算法的当前输出与与其输出之间的差距。衡量结果是一种反馈信号，用于调整算法的工作方式。这个调整步骤就是我们所说的**学习**。

机器学习模型将输入数据变换为更有意义的输出，这是一个从已知的输入输出示例中进行“学习”的过程。因此，机器学习和深度学习的核心问题在于**有意义地变换数据**，换句话说，在于学习输入数据的有用**表示**：这种表示可以让数据更接近预期输出。

什么是表示？这一概念的核心在于以一种不同的方式来查看数据。（表征数据或将数据编码）。举例来说，彩色图像可以编码为RGB（红、绿、蓝）格式或HSV格式（色相、饱和度、明度）格式，这些是对同一数据的两种表示。在处理某些任务时，使用某种表示可能会很困难，但换用另一种表示就会变得很简单。举个例子，对于“选择图像中所有的红色像素”这项任务，使用RGB格式会更简单，而对于“降低图像饱和度”这项任务，使用HSV格式则更简单。机器学习模型旨在为输入数据寻找合适的表示（对数据进行变换），使其更适合手头的任务。

对于简单的例子，利用人类智慧可以直接想出来适当的数据表示。但是如果任务更加复杂，比如对手写数字进行分类，难度会大很多。我们可以通过基于数字的规则（比如圆圈个数、垂直像素直方图和水平像素直方图）区分手写数字，但是手动寻找这样的有用表示是很困难的，而且可以想象，由此得到的基于规则的系统很脆弱，系统维护很困难。每当遇到一个新的手写数字不符合精心设计的规则是，你都不得不添加新的数据变换和新的规则，还要考虑它们与之前每条规则之间的相互作用。

这个过程能否自动化呢？

如果我们尝试系统地搜索自动生成的数据表示与基于这些表示的规则之间的不同组合，利用正确分类的数字所占百分比作为反馈信号，在某个开发数据集上找到那些好的组合，这样，我们做的就是机器学习。机器学习中的学习是指，寻找某种数据变换的自动搜索过程。

这种变换既可以是坐标变换，也可以是像素直方图和圆圈个数，还可以是线性投影、平移、非线性操作。机器学习算法在寻找这些变换时通常没有创造性，仅仅是遍历一组预先定义的操作，这组操作叫做假设空间。例如，所有可能的坐标变换组成的空间就是上述二维坐标分类示例的假设空间。

简而言之，机器学习就是指在预先定义的假设空间中，利用反馈信号的指引，在输入数据中寻找有用的表示和规则。

#### 1.1.4 深度学习之“深度”

深度学习是机器学习的一个分支领域：它是从数据中学习表示的一种新方法，强调从连续的层中学习，这些层对应于越来越有意义的表示。深度学习之“深度”并不是说这种方法能够获取更深层次的理解，而是指一系列连续的表示层。数据模型所包含的层数被称为该模型的深度。这一领域的其他名称还有分类表示学习（layered representations learning）和层级表示学习（hierarchical representations learning）。现代深度学习模型通常包含数十个甚至上百个连续的表示层，它们都是从训练数据中自动学习而来的。与之相对，其他机器学习方法的重点通常仅学习一两层的数据表示（例如获取像素直方图，然后应用分类规则），因此有时也被成为浅层学习。

在深度学习中，这些分层表示是通过叫做神经网络（neural network）的模型学习得到的。神经网络的结构是逐层堆叠。神经网络这一术语来自神经生物学，然而，深度学习模型并不是大脑模型。没有证据表明大脑的学习机制与现代深度歇息模型的学习机制相同。

深度学习算法学到的数据表示是什么样的呢？我们来看一个深度神经网络如何对数字图像进行变换，以便识别图像中的数字。

这个神经网络将数字图像变换为与原始图像差别越来越大的表示，而其中关于最终结果的信息越来越丰富。可以将深度神经网络看作多级信息蒸馏（information distillation）过程：信息穿过连续的过滤器，其纯度越来越高（对任务的帮助越来越大）。

#### 1.1.5 用三张图理解深度学习的工作原理

在神经网络中，每层对输入数据所作的具体操作保存在该层的权重中，权重实质就是一串数字。用术语来讲，每层实现的变换由其权重来参数化。权重又是也被成为该层的参数（parameter）。在这种语境下，学习的意思就是为神经网络的所有层找到一组权重值，使得该神经网络能够将每个示例的输入与其目标正确地一一对应。但问题来了：一个深度神经网络可能包含上千万个参数，找到其中所有参数的正确取值似乎是一项非常艰巨的任务，特别是考虑到修改一个参数将影响其他所有参数的行为。

若要控制某个事物，首先需要能够观察它。若要控制神经网络的输出，需要能够衡量该项输出与预期结果之间的距离。这是神经网络损失函数（loss function）的任务，该函数有时也被称为目标函数（objective function）或代价函数（cost function）。损失函数的输入是神经网络的预测值与真实目标值，输出是一个距离值，反应该神经网络在这个示例上的效果好坏。

深度学习的基本技巧是将损失值作为反馈信号，来对权重进行微调，以降低当前示例对应的损失值。这种调节是优化器（optimizer）的任务，它实现了所谓的反向传播（backpropagation）算法，这是深度学习的核心算法。

由于一开始对神经网络的权重进行随机赋值，因此神经网络仅实现了一系列随即变换，其输出值自然与理想结果相去甚远，相应地，损失值也很大。但是，神经网络每处理一个示例，权重值都会向着正确的方向微调，损失值也相应减小。这就是训练循环（traning loop），将这种循环重复足够多的次数（通常是对数千个示例进行数十次迭代），得到的权重值可以使损失函数最小化。具有最小损失值的神经网络，其输出值与目标值尽可能地接近，这就是一个训练好的神经网络。

#### 1.1.6 深度学习已经取得的进展

- 接近人类水平的图像分类
- 接近人类水平的语音识别
- 接近人类水平的手写文字识别
- 大幅改进的机器翻译
- 大幅改进的文本到语音转换
- 数字助理
- 接近人类水平的自动驾驶
- 更好的广告定向投放
- 更好的互联网搜索结果
- 能够回答用自然语言提出的问题
- 在下围棋时能战胜人类

随着每一个里程碑的出现，我们越来越接近这样一个时代：深度学习在人类从事的每一项活动和每一个领域中都能为我们提供帮助，包括科学、医学、制造业、能源、交通、软件开发、农业，甚至是艺术创作。

#### 1.1.7 不要相信短期炒作

虽然深度学习近年来取得了令人瞩目的成就，但人们对这一领域在未来十年里所能取得的成就似乎期望过高。虽然一些改变世界的应用（比如自动驾驶汽车）已经触手可及，但更多的应用可能在很长一段时间内难以实现，比如可信的对话系统、达到人类水平的跨任意语言的机器翻译，以及达到人类水平的自然语言理解。

#### 1.1.8 人工智能的未来

不要相信短期炒作，但一定要相信长期愿景。人工智能或许需要一段时间才能充分发挥其潜力，这一潜力大到难以想象，但人工智能时代终将到来，它将以一种奇妙的方式改变我们的世界。

### 1.2 深度学习之前：机器学习简史

当前工业界所使用的大部分机器学习算法并不是深度学习算法。深度学习并不一定总是解决问题的正确工具：有时没有足够的数据，深度学习不适用；有时用其他算法可以更好的解决问题。

关于机器学习方法的详细讨论已经超出了本书的范围，但我们将简要回顾这些方法，并了解这些方法发展的历史背景。这样，我们便可以将深度学习放入机器学习的大背景中，更好地理解深度歇息的起源和重要性。

#### 1.2.1 概率建模

概率建模（probabilistic modeling）是统计学原理在数据分析中的应用。它是最早的机器学习形式之一，至今仍在广泛使用。在概率建模中，最著名的算法就是朴素贝叶斯算法。

朴素贝叶斯是一类机器学习分类器，基于对贝叶斯定理的应用。它假设输入数据的特征都是独立的（这是一个很强的假设，或者说是“朴素”的假设，其名称正来源于此）。这种数据分析方法比计算机出现的还要早，在其第一次被计算机实现的几十年前就已经靠人工计算来应用了。

另一个密切相关的模型是logistic回归（logistic regression，简称logreg），它有时被认为是现代机器学习的“Hello World”。不要被它的名称所误导：logreg是一种分类算法，而不是回归算法。与朴素贝叶斯类似，logreg的出现也比计算机早很长时间，但由于它既简单又通用，因此至今仍然很有用。

#### 1.2.2 早期神经网络

神经网络的早期版本已经完全被本章介绍的现代版本所取代，但仍有助于我们了解深度学习的起源。虽然人们早在20实际50年代就开始研究神经网络及其核心思想，但这一方法在数十年后才被人们所使用。在很长一段时间里，一直没有训练大型神经网络的有效方法。这种情况在20世纪80年代中期发生了变化，当时有很多人独立地重新发现了反向传播算法，一种利用梯度下降优化来训练一系列参数化运算链的方法，并开始将其应用于神经网络。

神经网络的第一个成功的实际应用来自于1989年的贝尔实验室，当时Yann LeCun将卷积神经网络的早期思想与反向传播算法相结合，并将其应用于手写数字分类问题。

#### 1.2.3 核方法

神经网络取得了第一次成功，但一种新的机器学习方法在这时声名鹊起，这种方法就是核方法。核方法（kernel method）是一组分类算法，其中最有名的就是支持向量机（support vector machine, SVM）。

SVM的原理是寻找划分两个类别的决策边界。SVM通过以下两步来寻找决策边界：

（1）将数据映射到新的高维表示，此时决策边界可以用一个超平面来表示。

（2）尽量让超平面与每个类别最近的数据点之间的距离最大化，从而计算出良好的决策边界（分离超平面）。这样一来，决策边界便可以很好地推广到训练数据集之外的新样本。

将数据映射到高维表示从而使分类问题简化，这一方法可能听起来不错，但实际上计算起来很棘手。这是就需要用到核技巧（kernel trick）。核技巧的基本思想是：要在新的表示空间中找到良好的决策超平面，不需要直接计算点在新空间中的坐标，只需要计算在新空间中点与点之间的距离，而利用核函数可以高效地完成这种计算。核函数是一个在计算上容易实现的运算，它将初始空间中的任意两点映射为这两点在目标空间中的距离，从而完全避免了直接计算新的表示。核函数通常是认为选择的，而不是从数据中学到的。对于SVM来说，只有分离超平面是通过学习得到的。

SVM刚刚出现时，在简单的分类问题上表现出非常好的性能。当时只有少数机器学习方法得到大量的理论支持，并且经得起严谨的数学分析，非常易于理解核解释，SVM就是其中之一。由于SVM具有这些有用的性质，因此它在很长一段时间里非常流行。

但事实证明，SVM很难扩展到大型数据集，并且在图像分类等感知问题上的效果也不好。SVM是一种浅层方法，因此要将其应用于感知问题，首先需要手动提取出最有用的表示（这一步骤叫做特征工程）。这一步骤很难，而且不稳定。如果想用SVM来进行手写数字分类，那么不能从原始像素开始，而应该首先手动找到有用的表示（比如像素直方图），使问题变得更易于处理。

#### 1.2.4 决策树、随机森林核梯度提升机

决策树（decision tree）是类似于流程图的结构，可以对输入数据进行分类或根据输入预测输出值。决策树的可视化和解释都很简单，到了2010年，决策树往往比核方法更受欢迎。

随机森林（random forest）算法引入了一种稳健且实用的决策树学习方法，即首先构建许多专门的决策树，然后将它们的输出集成在一起。随机森林适用于各种各样的问题。对于任何浅层的机器学习任务来说，它几乎总是第二好的算法。广受欢迎的机器学习网站Kaggle在2010年上线后，随机森林迅速成为该平台用户的最爱，直到2014年才被梯度提升机（gradient boosting machine）所取代。与随机森林类似，梯度提升机也是将弱预测模型进行集成的机器学习技术。它使用了梯度提升方法，这种方法通过迭代地训练新模型来专门弥补原有模型的弱点，从而可以提升任何机器学习模型的效果。将梯度提升技术应用于决策树时，得到的模型与随机森林具有相似的性质，但在绝大多数情况下效果更好。它可能是目前处理非感知数据最好的算法之一。

#### 1.2.5 回到神经网络

虽然神经网络曾几乎被整个科学界忽略，但仍有一些人在坚持研究神经网络，并在2010年前后开始取得重大突破。

2011年，来自IDSIA的Dan Ciresan开始利用GPU训练的深度神经网络赢得学术性图像分类比赛，这是现代深度学习第一次在实践中取得成功。但真正的转折点出现在2012年，Hinton小组在这一年参加了每年一次的大规模图像分类挑战赛ILSVRC（ImageNet大规模视觉识别挑战赛）。当时该挑战赛以困难著称，参赛者需要对140万张高分辨率彩色图像进行训练，然后将其划分到1000个类别中。在2011年获胜的模型基于经典的计算机视觉方法，其top-5精度只有74.3%。到了2012年，有Alex Krizhevsky带领并由Geoffrey Hinton提供建议的小组实现了重大突破，达到了83.6%的top-5精度。此后，这项比赛每年都被深度卷积神经网络所主导。到了2015年，获胜模型的精度达到了96.4%，此时ImageNet的分类任务被认为是一个完全解决的问题。

自2012年以来，深度卷积神经网络（convnet）已成为所有计算机视觉任务的首选算法。更一般地说，它适用于所有感知任务。与此同时，深度歇息也在许多其他类型的问题上得到应用，比如自然语言处理。它已经在大量应用中完全取代了SVM决策树。

#### 1.2.6 深度学习有何不同

深度学习发展得如此迅速，主要原因在于它在很多问题上表现出更好的性能，但这并不是唯一的原因。深度学习还让解决问题变得更加简单，因为它将特征工程完全自动化，而这曾经是机器学习工作流程中最关键的一步。

先前的机器学习技术（浅层学习）仅涉及将输入数据变换到一两个连续的表示空间，通常使用简单的变换，比如高维非线性投影（SVM）或决策树。但这些技术通常无法得到复杂问题所需要的精确表示。因此，人们不得不竭尽所能让初始输入数据更适合于用这些方法处理，并不得不手动为数据设计良好的表示层。这一步叫做特征工程。与此相对，深度学习将这一步完全自动化：利用深度学习，可以一次性学习所有的特征，而无须自己手动设计。这极大地简化了机器学习工作流程，通常用一个简单、端到端的深度学习模型取代复杂的多级流程。

你可能会问，如果问题的关键在于有多个连续表示层，那么能否重复应用浅层方法，以实现与深度学习类似的效果呢？在实践中，如果连续应用浅层学习方法，那么其收益会随着层数增加而迅速降低，因为三层模型中最有的第一表示层并不是单层模型或双层模型中最优的第一表示层。深度学习的变革之处在于，模型可以在同一时间共同学习所有表示层，而不是依次连续学习（这被称为贪婪学习）。通过共同的特征学习，每当模型修改某个内部特征时，所有依赖于该特征的其他特征都会相应地自动台哦姐适应，无须认为干预。一切都由单一反馈信号来监督：模型中的每一处变化都是为最终目标服务。这种方法比贪婪地叠加浅层模型更强大，因为它可以通过将复杂、抽象的表示拆解为多个中间空间来学习这些表示，每个中间空间仅仅是前一个空间的简单变换。

深度学习从数据中进行学习时有两个基本特征：第一，通过逐层渐进的方式形成越来越复杂的表示；第二，对中间这些渐进的表示共同进行学习，每一层的修改需要同时考虑上下两层。这两个特征会叠加在一起，使得深度学习比先前的机器学习方法更成功。

#### 1.2.7 机器学习现状

若要了解机器学习算法和工具的现状，一个很好的方法是查看Kaggle上的机器学习竞赛。

### 1.3 为什么要用深度学习，为什么是现在

深度学习用于计算机视觉的两个关键思想，即卷积神经网络和反向传播，在1990年就已经被人们所熟知。长短期记忆（LSTM）算法是深度学习处理时间序列的基础，它在1997年就被开发出来，而且此后几乎没有变化。那么为什么深度学习在2012年之后才开始取得成功？

总体来说，有3股技术力量推动着机器学习的进步：

- 硬件
- 数据集和基准
- 算法改进

由于这一领域是靠实验结果而不是靠理论指导的，因此只有当合适的数据和硬件可用于尝试新想法时，才可能出现算法上的改进。与数学或物理学不同，仅靠一支笔和一张纸不能实现机器学习的重大进展，它是一门工程科学。

#### 1.3.1 硬件

CPU

GPU

TPU（专业芯片，张量处理器）

#### 1.3.2 数据

就数据而言，除了过去20年里存储硬件的指数级增长，最大的变革来自于互联网的兴起，它使得收集与分发用于机器学习的超大型数据集变得可行。

#### 1.3.3 算法

除了硬件和数据，直到21世纪前10年末，我们仍然没有可靠的方法来训练非常深的神经网络。因此神经网络仍然很浅，仅使用一两个表示层，无法超越更为精确的浅层方法，比如SVM和随机森林。关键的问题在于通过多层叠加的梯度传播。随着层数的增加，用于训练神经网络的反馈信号会逐渐消失。

这一情况在2009年-2010年发生了变化，当时出现了几个很简单但很重要的算法改进，从而可以实现更好的梯度传播。

- 更好的神经层激活函数
- 更好的权重初始化方案，一开始使用逐层预训练的方法，不过这种方法很快就被放弃了
- 更好的优化方案，比如RMSprop和Adam

2014-2016年，人们发现了更先进的梯度传播改进方法，比如批量规范化、残差连接和深度可分离卷积。

今天，我们可以从头开始训练任意深度的模型。这使我们可以使用极其庞大的模型，这些模型拥有相当强大的表示能力，也就是说，它们可以编码非常丰富的假设空间。这种极强的可扩展性时现代深度学习的一大典型特征。

#### 1.3.4 新一轮投资热潮

#### 1.3.5 深度学习的普及

在早期，从事深度学习需要精通C++和CUDA，而只有少数人才拥有这些技能。

如今，只要具有基本的Python脚本技能，就足以从事高级的深度学习研究。这主要得益于Theano库（已停止开发）与其后的TensorFlow库的开发，以及Keras等用户友好型库的兴起。

#### 1.3.6 这种趋势会持续下去吗

深度学习有几个重要的性质，可以证明它确实是人工智能的革命，并且能长盛不衰。二十年后，我们可能不再使用神经网络，但我们那是所使用的工具都会直接集成自现代深度学习及其核心概念。这些重要的性质大致可以分为以下3类：

- 简单。深度学习不需要特征工程，它将复杂、不确定、工程量很大的流程替换为简单、端到端的可训练模型，构建这种模型通常只需要5、6种张良运算。
- 可扩展。深度学习非常适合在GPU或TPU上并行计算，因此可以充分利用摩尔定律。此外，深度学习模型通过对小批量数据进行迭代来训练，因此可以在任意规模的数据集上进行训练。
- 通用、可复用。与之前许多机器学习方法不同，深度学习模型无须从头开始就可以在新增的数据上进行训练，因此可用于连续在线学习，这对于大型生产模型而言是非常重要的特性。此外，训练好的深度学习模型是可以重复使用的。举个例子，可以将一个对图像分类进行训练的深度学习模型应用于视频处理流程。这样一来，我们可以将精力投入到日益复杂和强大的模型中。这也使得深度学习适用于较小的数据集。

## 2 神经网络的数学基础

要理解深度学习，需要熟悉很多简单的数学概念：张量、张量运算、微分、梯度下降等。

### 2.1 初始神经网络

我们来看一个神经网络的具体实例：使用Python的Keras来学习手写数字分类。

在这个例子中，我们要解决的问题是，将手写数字的灰度图像划分到10个类别中。我们将使用MNIST数据集，它是机器学习领域的一个经典的数据集，其历史几乎和这个领域一样长，而且已经被深入研究。这个数据集包含60000张训练图像和10000张测试图像，由美国国家标准与技术研究院（NIST）在20实际80年代收集而成。

ref: c2_1.py

### 2.2 神经网络的数据表示

在前面的例子中，我们的数据存储在多维NumPy数组中，也叫作张量（tensor）。一般来说，目前所有的机器学习系统都是用张量作为基本数据结构。

张量这一概念的核心在于，它是一个数据容器。它包含的数据类型通常都是数值数据，因此它是一个数字容器。你可能对矩阵很熟悉，它是2阶张量。张量是矩阵向任意维度的推广（注意，张量的维度通常叫做轴）。

#### 2.2.1 标量（0阶张量）

仅包含一个数字的张量叫做标量（scalar），也叫标量张量、0阶张量或0维张量。在NumPy中，一个float32类型或float64类型的数字就是一个标量张量（或标量数组）。可以用ndim属性来查看NumPy张量的轴的个数。标量张量有0个轴。张量轴的个数也叫作阶（rank）。下面是一个NumPy标量。

```python
# c_2_1.py
import numpy as np
x = np.array(12)
print(x) # 12
print(x.ndim) # 0
```

#### 2.2.2 向量（1阶张量）

数字组成的数组叫做向量（vector），也叫1阶张量或1维张量。1阶张量只有一个轴。下面是一个NumPy向量。

```python
# c_2_2.py
import numpy as np
x = np.array([12, 3, 6, 14, 7])
print(x)
print(x.ndim)
```

这个向量包含5个元素，所以叫做5维向量。注意5维向量和5维张量是两个概念。维度既可以表示沿着某个轴上的元素的个数，也可以表示张量的轴的个数，这有时让人困惑。张量的轴的个数，更准确的术语是5阶张量。

#### 2.2.3 矩阵（2阶张量）

向量组成的数组叫做矩阵，也叫作2阶张量或2维张量。矩阵有2个轴（通常叫做行和列）。下面是一个NumPy矩阵。

```python
# c2_2_3.py
import numpy as np
x = np.array([[5, 78, 2, 34, 0],
             [6, 79, 3, 35, 1],
             [7, 80, 4, 36, 2]])
print(x)
print(x.ndim) # 2
```

#### 2.2.4 3阶张量与更高阶的张量

将多个矩阵打包成一个新的数组，就可以得到一个3阶张量。可以直观地理解为数字组成的立方体。

```python
# c2_2_4.py
import numpy as np
x = np.array([[[5, 78, 2, 34, 0],
             [6, 79, 3, 35, 1],
             [7, 80, 4, 36, 2]],
             [[5, 78, 2, 34, 0],
             [6, 79, 3, 35, 1],
             [7, 80, 4, 36, 2]],
             [[5, 78, 2, 34, 0],
             [6, 79, 3, 35, 1],
             [7, 80, 4, 36, 2]]])
print(x)
print(x.ndim) # 3
```

将多个3阶张量打包成一个数组，就可以创建一个4阶张量，一次类推。深度学习处理的一般是0到4阶的张量，但处理视频数据时可能会遇到5阶张量。

#### 2.2.5 关键属性

张量是由一下3个关键属性来定义的：

- 轴的个数（阶数）。
- 形状。这是一个整数元组，表示张量沿每个轴的维度大小（元素个数）。举例来说，前面的矩阵示例的形状维（3，5），3阶张量示例的形状为（3，3，5）。向量的形状只包含一个元素，比如（5，），而标量的形状为空，即（）。
- 数据类型（在Python库中通常叫做dtype）。这是张量中所包含数据的类型。举例来说，张量的类型可以是float16、float32、float64、uint8等。在TensorFlow中，你还可能遇到string类型的张量。

回顾一下在MNIST例子中处理的数据。

```python
from keras.datasets import mnist

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

print(train_images.ndim) # 3
print(train_images.shape) # (60000, 28, 28)
print(train_images.dtype) # uint8
```

其中train_images的轴的个数为3，形状是(60000, 28, 28)，数据类型是uint8。

也即，train_images是一个由8位整数组成的3阶张量。更确切地说，它是由60000个矩阵组成的数组，每个矩阵由28x28个整数组成。每个这样的矩阵都是一张灰度图像，元素取值在0-255之间。

我们用Matplotlib库来显示这个3阶张量中的第4个数字。

```python
from keras.datasets import mnist
import matplotlib.pyplot as plt

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

print(train_images.ndim) # 3
print(train_images.shape) # (60000, 28, 28)
print(train_images.dtype) # uint8

digit = train_images[4]
plt.imshow(digit, cmap='binary')
plt.savefig('plot.png')

print('label: ', train_labels[4])
```

#### 2.2.6 在NumPy中操作张量

在前面的例子中，我们使用语法`train_images[i]`来沿着第一个轴选择某张数字图像。选择张量的特定元素叫作张量切片。我们来看一下NumPy数组可以做哪些张量切片运算。



