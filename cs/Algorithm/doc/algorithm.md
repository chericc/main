# Introduction to Algorithms - Third Edition

## 第一部分 基础知识

### 1.1 第一章 算法在计算中的作用

### 1.2 第二章 算法基础

#### 循环不变式

将算法中每一次处理完成后，具有相同性质的对象称为循环不变式。  

循环不变式若具备以下三条性质，则通过数学归纳法就能推导出算法正确：  

1. 初始化：循环的第一次迭代之前，循环不变式为真。  
2. 保持：如果循环的某次迭代之前循环不变式为真，那么下次迭代之前循环不变式为真。  
3. 终止：在循环终止时，不变式为我们提供一个有用的性质，该性质有助于证明算法是正确的。  

以对 $n$ 个元素进行插入排序为例：  

代码参考：chapter1/sort.c sort_insert  
插入排序中，以待处理元素左侧的有序数列为循环不变式。现在按循环不变式的要求来证明插入算法的正确性：  

1. 初始化：第一次执行时，左侧数列只有一个元素，必然有序，即循环不变式为真。  
2. 保持：假设上一次循环时，有左侧数列有序，则处理新的元素时，由于从右到左依次对比该元素小的项进行了交换，因此结束时一定有该元素不大于左侧元素，且不小于右侧元素，也即最终形成的序列也是有序的，也是一个循环不变式。  
3. 终止：当处理的元素索引在数列以外时，循环终止，此时数列的全部元素是一个循环不变式。由于循环不变式是一个有序数列，因此算法结束时，真个数列是有序的，实现了算法的目标。  

#### 算法分析

**分析模型**：以一种通用的单处理器计算模型，也即随机访问机（random-access machine, RAM）来进行分析。  
RAM 模型具有以下特点：  

- 指令一条接一条执行，没有并发（重叠）操作。  
- 指令是真实计算机中的常见指令，并且每条指令执行的时间都为常量。  
- 不考虑内存的层次（不考虑高速缓存）。  
- 不考虑数据类型的精度。  

**平均运行时间和最差运行时间**：一般情况下，只考虑一个算法的最差运行时间，因为最差运行时间不依赖于算法的具体使用场景，给出了算法的效率上界。某些特殊的情况下（如输入具有随机性），可以考虑一个算法的平均运行时间。  

**增长量级**：对于算法的运行时间，最核心的参数是其增长量级。比如一个算法的运行时间是一个关于输入规模的多项式，则以多项式的最高次数项去掉常数项后得到的因子来衡量算法的增长量级。  

### 1.3 第三章 函数的增长

几种符号的定义：  

$\varTheta(g(n))$  
若存在正常量 $c_1,c_2$ ，当 $n$ 足够大时，能满足 $c_1g(n)\le f(n) \le c_2g(n)$ ，则称 $f(n)\in \varTheta(g(n))$ 。此时，称 $g(n)$ 是 $f(n)$ 的一个渐进紧确界。  
渐进紧确界既给出了渐进上界也给出了渐进下界。  

$O(g(n))$  
若存在常量 $c$ ，当 $n$ 足够大时，能满足 $f(n)\le cg(n)$ ，则称 $f(n)\in O(g(n))$ 。此时，称 $g(n)$ 是 $f(n)$ 的一个渐进上界。  

例子：如果说一个算法的运行时间为 $O(n^2)$ ，即表示当 $n$ 足够大时，存在一个函数 $cn^2$ ，满足算法的执行时间 $t(n)\le cn^2$ 。  

### 1.4 第四章 分治策略

**分治策略**可以通过递归实现，每层递归中应用如下三个步骤：  

- 分解：将问题花粉为一些子问题，子问题的形式和原问题一样，只是规模更小。  
- 解决：递归地求解出子问题。如果自问题的规模足够小，则停止递归，直接求解。  
- 合并：将子问题的解组合成原问题的解。  

在用递归的方法进行求解时，如果问题需要分解，则称之为**递归情况**。如果问题足够小，不需要分解为子问题，则称为**基本情况**。  

**递归实现界的求法**：通过递归的方法进行分治求解时，主要有三种方法求出算法的运行时间的界。  

- 归纳法：先猜测一个界，然后通过归纳法进行证明。  
- 递归树法：将递归层次抽象为一课递归树，通过树的高度和树每层的运行时间求出界。  
- 公式法：求出递归式，然后求出界。  

#### 1.4.1 通过分治算法求解最大子数组问题

**问题实例**：已知某股票在一段时间内的价格，要确定一个买入时间和卖出时间，使得最终受益最大。  

**问题抽象**：将股价抽象为一个数组，即要求一个子数组，使得子数组的末尾项和首项的差值最大。称这种问题为最大子数组问题。  

**穷举法（Exhaustive Method）**：列举出所有可能的子数组，找出这些子数组中的最大子数组。  

**分治法**：将数组分为左右两部分，则最大子数组只可能在左侧子数组、右侧子数组中，或者横跨两个子数组。  
时间复杂度：对数组进行均分，因此高度为 $\log_2n$ ，每个递归中复杂度为横跨两个子数组的时间，为 $O(n)$ ，因此总的时间复杂度为 $O(n\log_2 n)$

代码示例：chapter4/largest_sub_array.cpp  

#### 1.4.2 矩阵乘法的 Strassen 算法

**矩阵乘法定义的直接程式化**：即根据 $\displaystyle c_{ij}=\sum_{k=1}^{n} a_{ik}b_{kj}$ 计算矩阵乘法结果的每个元素的值，从而求得结果矩阵。  

由于矩阵需要额外的数据结构定义，这里代码示例建立为工程，参考 project/Matrix 。  

**矩阵乘法的直接分治法**：矩阵乘法的分治法利用矩阵乘法的分块法原理实现。矩阵乘法的分块法引述如下：  
$\boldsymbol{A}=( \boldsymbol{A}_{ij})_{ab}$ ，$\boldsymbol{B}=(\boldsymbol{B}_{ij})_{bc}$ ，如果 $\boldsymbol{A}_{i1},\boldsymbol{A}_{i2},...,\boldsymbol{A}_{ib}$ 和 $\boldsymbol{B}_{1j},\boldsymbol{B}_{2j},...,\boldsymbol{B}_{bj}$ 分别对应可乘，则 $\boldsymbol{AB}=\boldsymbol{C}=(\boldsymbol{C}_{ij})_{ac}$ ，其中 $\displaystyle \boldsymbol{C_{ij}}=\sum_{k=1}^{b}\boldsymbol{A}_{ik}\boldsymbol{B}_{kj}$ 。  

直接分治算法中，我们使用了大量的数据复制，因此时间复杂度远大于乘法定义的程式实现。

#### 1.4.3 用递归方程求解时间复杂度

设递归方程为 $T(n)=aT(n/b)+f(n)$ ，则 $T(n)$ 有如下渐进界：  
1. 若对某个常数 $\varepsilon>0$ 有 $f(n)=O(n^{\log_b (a-\varepsilon)})$ ，则 $T(n)=\Theta(n^{\log_b a})$  
2. 若 $f(n)=\Theta (n^{\log_b a})$ ，则 $T(n)=\Theta (n^{\log_b a\lg n})$  
3. 若对某个常数 $\varepsilon>0$ 有 $f(n)=\Omega (n^{\log_b (a+\varepsilon)})$ ，且对某个常数 $c<1$ 和所有足够大的 $n$ 有 $af(n/b)\le cf(n)$ ，则 $T(n)=\Theta (f(n))$  

这个定理暂不证明。  
这个定理的应用暂不扩展。  

### 1.5 第五章 概率分析和随机算法

略。

## 第二部分 排序和顺序统计量

**排序**，即对于一个给定的数列，对所有元素进行重排，使得数列中任意左侧的元素不大于右侧的元素。  

**顺序统计量**，即找出 $n$ 个数中第 $i$ 小的数。  

### 2.6 第六章 堆排序

**堆**是一个数组，对应一棵完全二叉树。其中，数组从第一个元素到最后一个元素，依次对应二叉树的根节点和从上往下，从左至右的节点。  

**原理**：构造最大堆，然后循环进行根元素和堆尾部元素交换，并执行堆维护，直到堆中只剩余一个元素。  

堆维护过程：  
对于一个最大堆，如果交换了堆尾元素和根元素，则堆的性质被破坏，此时只需要将根元素逐层下降，每次下降时分别与左右子树的根节点进行比较，并与较大者进行交换。  
堆维护与树的高度有关，因此其时间复杂度为 $O(\lg n)$ 。  

建堆过程：  
建堆可以利用堆维护过程来进行，从下至上，从右至左（左右顺序其实没关系），依次对每一个子树执行堆维护过程，则最终就能得到一个最大堆。  
不失一般性，以一个满树为例，设树高为 $h$ ，则总的时间复杂度为 $h + 2 (h-1) + 2^2(h-2)+...+2^{h-1}\cdot (h-(h-2))=S$ ，可以求得 $S=h+2^h+2^{h-1}$ ，因此有时间复杂度为 $O(n)$ 。  

#### 堆排序

仅以树的最下层元素的排序为例，一共 $\lg n$ 个元素，每个元素需要 $O(n)$ 的时间来维护堆，因此总的时间复杂度为 $n\lg n$ 。  

代码示例：  
理解了堆排序的维护过程和建堆过程之后，堆排序并不难理解。  

代码参考：chapter6/heapsort.cpp

#### 优先队列

优先队列同样可以利用最大堆来实现。  

取最大元素：  
直接返回根元素即可。  

取最大元素并删除最大元素：  
取得根元素，并将最后一个元素和根元素交换。  
然后执行对维护过程。

扩大某元素：  
找出该元素和其父节点的大者，并将该节点和大者交换。如果
父节点被交换，则继续堆父节点进行检查。  

缩小某元素：  
和堆维护过程一致。  

插入新元素：  
在堆末尾新增一个空元素，并将该元素
缩小或扩大至给定的值。  

代码示例：  
和堆排序类似，略。  

### 2.7 第七章 快速排序

快速排序的核心思想是对给定的一组数，选定一个元素并以此为参考，将元素划分为小于参考的元素和大于参考的元素。  

代码示例：  
chapter6/quicksort.cpp

### 2.8 第八章 线性时间排序

#### 决策树模型

通过比较来排序可以被抽象为一个决策树模型，这个决策树的每一个节点表示一次比较，决策树的左右分支表示比较的结果。  
决策树的高度表示了比较的最大次数，决策树的所有叶节点表示所有可能的排序结果。也即每个叶节点对应一个原始的排列。  

由于 $n$ 个元素最多有 $n!$ 个排列，高度为 $h$ 的树最多有 $2^h$ 个叶节点，因此有 $n! < 2^h$ ，也即 $h>\lg_2 n!$ ，也即比较排序有下界 $n\lg n$ 。  

#### 计数排序

以待排元素的值作为索引，放置到对应存储空间中，然后从存储空间中按地址顺序找到所有元素，即为计数排序。  

#### 基数排序

分别对一个进制数的各个位依次排序，并且使用计数排序排序实现。  

#### 桶排序

将浮点数的范围映射为桶，和计数排序类似的方法进行排序。  

### 2.9 第九章 中位数和顺序统计量

#### 最小值和最大值

求出最小值或者最大值，最少需要比较 $n$ 次。  

同时求出最小值和最大值，可以将元素划分为 $2$ 个一组，将两个元素先进行比较，则 $2$ 个元素只需要 $3$ 次比较，也即总的比较次数为 $3n/2$ 次。  

#### 选择问题

选择问题：  
找出 $n$ 个互异的数中第 $i$ 大的数。  

同找出最大值和最小值一样，选择问题也是一个可以在线性时间内解决的问题。  

使用分治法解决选择问题：  

选定一个元素作为主元，将输入划分为两个部分，若主元恰为第 $i$ 个元素，则直接返回主元；若第 $i$ 个元素在左侧，则对左侧元素继续划分；若第 $i$ 个元素在右侧，则对右侧元素进行划分。  



## 第三部分 数据结构

### 第10章 基本数据结构

#### 栈和队列

**栈**实现的是一种先进后出的策略；**队列**实现的是一种先进先出的策略。  

#### 链表

链表是一种各对象按现行顺序排列的数据结构，链表中对象的顺序是由各个对象里的指针决定的。  

#### 有根树

链表的表示进行推广，即一个节点可以通过多个指针指向多个对象，即实现了一个有最大分支数量的有根树。  

对于分支没有限制的有根树，可以通过左孩子，右兄弟的表示方法，转换为一棵二叉树表示。  

### 第11章 散列表

数组可以看作是一种特殊的散列表。  

如果关键字和值的位置一一对应，则就是直接寻址。  
如果关键字和值的位置通过一个散列函数对应，则就是散列表形式。  

#### 链接法

通过散列函数来对值的位置进行散列，存在冲突的可能。  

链接法是一种解决冲突的手段。如果出现了冲入，则在冲突位置引入链表进行解决。  

#### 除法散列

$h(k)=k \mod C$ ，除数本身就是散列表的容量大小。  
除数应避免一些特定的值，比如如果选择为 $2$ 的 $n$ 次幂，则 $h(k)$ 只会取 $k$ 的低 $n$ 位。  
一般而言，可以考虑一个远离 $2^n$ 的素数。  

#### 全域散列

所谓全域散列，即在散列函数的选择上引入随机性。  

#### 开放寻址法

与链接法对应，开放寻址法在碰到冲突之后，不是仍然使用原地址，而是在已有空间中按某种策略继续寻找可用位置。  

寻找的策略包括线性探查、二次探查、双重散列。  

线性探查：  
$h(k,i)=[h'(k) + i]\mod m$  
线性探查会引起一次群集。（即长度越长的已占用序列前的空槽被占用的概率越高）  

二次探查：  
$h(k,i)=[h'(k)+c_1i+c_2i^2]\mod m$  
二次探查会引起二次群集。二次群集的群集程度好于一次群集。  

双重散列：  
$h(k,i)=[h'(k)+ih''(k)]\mod m$  
双重散列是比较好的散列方式，但是为了确保所有的位置都能被散列到，要求 $h''(k)$ 和 $m$ 互素（互质，即互相不能整除）。  
一个实现方法是将 $m$ 取为素数，并将 $h''(k)$ 设计为总是返回比 $m$ 小的函数。  

### 第12章 二叉搜索树

构造一棵二叉树，并且始终保持左子节点不大于根节点，右子节点不小于根节点。  

BST：Binary search tree.  

查询：从根节点开始，按大小寻找。  

插入：和查询类似，找出待插入的位置。  

前驱：左子树的最右侧子节点；如果没有左子树，则向上查找第一个向左的双亲节点；如果没有向左的双亲节点，则没有前驱。  

后继：右子树的最左侧子节点；如果没有右子树，则向上查找第一个向右的双亲节点；如果没有向右的双亲节点，则没有后继。  

删除：  
- 如果没有孩子节点，则直接删除。  
- 如果只有一个子节点，则直接用子节点替换被删除的节点。  
- 如果有两个子节点，这里既可以从左子树中找节点来替换，也可以在右子树中找节点来替换。这里以右子树为例。    
  - 如果后继即为右子树，则直接用右子树替换。  
  - 如果后继不为右子树（则一定为右子树的最左侧节点），则先用后继的右节点替换后继，然后用后继来替换被删除的节点。  

代码示例：
src/project/BST

### 第13章 红黑树

红黑树是平衡搜索树中的一种，可以保证在最坏情况下基本动态集合操作的时间复杂度为 $O(\lg n)$ 。  

#### 红黑树的来源

注意：红黑树是一种对于 B 树（特殊的如 2-3-4 树）的转换。  
参考：<https://www.cnblogs.com/tiancai/p/9072813.html>  

2-3-4 树操作回顾：

注：所谓 2-3-4 树，也即 4 阶 B 树，是指一个节点可能的子节点数为 2 到 4 个。  
**查找**：和二叉搜索树类似，从根节点按大小逐个递归查找即可。  
**插入**：和二叉搜索树的插入逻辑类似，将节点插入到可列节点的叶节点上。与之不同的是，插入关键字总是插入到一个叶节点上（二叉搜索树是插入到叶节点的左右子节点上）。如果一个叶节点插入后关键字数超过最大个数，则将该叶节点分裂，并将中间的关键字移动到叶节点的双亲节点上。然后递归的对双亲节点进行检查。  
**删除**：删除的情况稍复杂一些：  
- 若被删除的节点不是叶节点，则其一定有右子树。取右子树的最小节点并替换被删除的节点，然后执行删除右子树的最小节点的操作（即删除一个叶节点）。  
- 若被删除的节点是叶节点，则一共有三种情况：  
  - 被删节点的关键字仍足够，则删除完成。  
  - 被删节点的关键字不够，但是其相邻兄弟节点的有可借的关键字，则用被删节点的双亲节点的临近关键字补充被删关键字的位置，然后用相邻节点的相邻关键字补充双亲节点被移除的关键字。  
  - 被删节点的关键字不够，且其相邻兄弟节点也没有可借关键字，则将双亲节点中的对应关键字移动到兄弟节点中。此时双亲节点被移除了一个关键字，则递归对双亲节点进行检查（如果双亲节点的关键字不够，则又考虑从双亲节点的兄弟节点借），直到双亲节点没有出现被删除的情况（少一个元素的情况），或者双亲节点作为根节点被删除（此时根节点替换为被合并后的节点）。  

2-3-4 树存在 2- 、3- 和 4- 三种节点。  

红黑树是一种 2-3-4 树的实现，其主要思路是将 2-3-4 树中的 3- 或 4- 节点利用着色的方式展开为 2- 节点树，展开的方式如下：  
对 3- 节点，将左侧关键字降为右侧元素的左子节点，3- 节点的三个指针分别成为被降为子节点的左侧关键字的左右子树，以及右侧元素的右侧指针。  
对 4- 节点，将左侧和右侧关键字降为中间元素的左子节点和右子节点，将 4- 节点的四个子节点分别分配给被降级为左右两个子节点的子节点。  
降级为子节点的元素染为红色，未降级的元素染为黑色。称未降级的节点为真实节点，降级的节点为扩展节点。  
这样，所有的节点就都是 2- 节点了，只是通过颜色的不同来标记是 2- 节点还是 3- 节点或 4- 节点。  

一些问题：  

- 如何判别节点是真实节点还是扩展节点？  
所有黑色节点一定都是真实节点，所有红色节点一定都是扩展节点。扩展节点从属的真实节点就是其双亲节点。  

#### 红黑树的性质

一棵**红黑树**是满足以下性质的**二叉搜索树**：  

1. 每个节点或是红色的，或是黑色的
2. 根节点是黑色的
3. 每个叶节点是黑色的
4. 如果一个节点是红色的，则它的两个子节点都是黑色的
5. 对每个结点，从该节点到其所有后代的简单路径上，均包含相同数目的黑色节点

由红黑树的性质可知，从根节点到所有叶节点的路径上的黑色节点数是相同的，因此记某一条路径上的黑色节点数为**黑高**。  

定理 一棵有 $n$ 个内部节点的红黑树的高度至多为 $2\lg (n+1)$  

使用归纳法证明：  
记节点 $x$ 的黑高为 $bh(x)$ ，假设有 $x$ 对应子树的内部节点数至少为 $2^{bh(x)}-1$ 。  
对叶节点，其黑高为 $0$ ，其有 $0$ 个内部节点，符合假设。  
对某个内部节点，其对应的子树为左子树和右子树的节点总数加 $1$ ，根据假设，即为 $2^{bh(x)-1}-1 + 2^{bh(x)-1} - 1 + 1=2^{bh(x)}-1$ ，也即符合假设。  
$n \ge 2^h-1$ ，也即 $h\le \lg_2(n+1)$  
结论对根节点也成立，因此得证。  

红黑树 5 个性质的理解：  
从 2-3-4 树的角度来理解。  

1. 只需要两种颜色来判别节点的类型。  
2. 由于根节点一定是真实节点，因此根节点一定是黑色节点。  
3. 叶节点用来处理边界条件，叶节点可以视为一个空节点。空节点显然是一个真实节点，因此为黑色。  
4. 由于红色节点的双亲节点是其真实节点，因此红色节点的双亲节点只能是黑色节点。  
5. 由于 2-3-4 树是一棵平衡树，因此所有叶节点的高度是相等的。在将 2-3-4 树转换为红黑树后，对于每个节点，转换后都只有一个黑色节点，也即每个节点涉及的路径中都只有一个黑色节点（或者说，只考虑 2-3-4 树的某一个节点，在转换后产生的所有路径上，都没有新增黑色节点），这样通过递归法就可以证明最终所有路径上的黑色节点的个数是相等的。  

性质的扩展：  
1. 红色节点的双亲节点一定是黑色节点。  

#### 红黑树的左右旋

左、右旋  
如果一个节点有右子树，则该节点能进行左旋；如果一个节点有左子树，则该节点能进行右旋。  
左、右旋是对称的操作。  

左旋和右旋，对于 2-3-4 树，也是旋转操作。  

#### 插入  

- 插入的节点被着色为红色  
分析：由于被插入的节点一定是插入到 2-3-4 树的某个节点中，也即对应的是一个扩展节点，因此一定是红色的。  

- 插入新的节点后如何维护？  
分析：  
这里我们先直接引入《算法导论》中介绍的操作。  
  1. 按二叉搜索树的插入逻辑，将新插入的节点插入红黑树，并将新插入的节点着色为红色。  
  2. 插入一个新节点只可能破坏规则 2 和 规则 4：  
  - 如果规则 2 被破坏，也即插入的节点就是根节点，则直接将其着色为黑色即可。  
  - 如果规则 4 被破坏，也即插入的节点的双亲节点也是红色节点，  
    现只考虑被插入的节点的父节点是左节点的情况（即新节点的父节点是左节点，或者说定位到的节点是左节点，此时只有两种情况，即新插入的红色节点是一个红色节点的左节点或右节点）  
    - 如果红色节点的上两层黑节点右节点也是红色节点，则将第一个黑节点的左右两个子节点都着色为黑色，并将第一个黑节点着色为红色，然后递归检查这个被着色的节点。  
    - 如果不满足之前的条件，且红色节点是一个右节点，则对该节点及其双亲节点执行左旋。  
    - 如果不满足之前的条件（即插入节点的上两层节点的右节点是叶节点），则将上层节点着为黑色，上两层节点着为红色，然后对上两层节点执行右旋。  
    - 注意：只有这三种情况。  
      也即，或者新插入的节点的上两层节点有右节点；或者上两层节点没有右节点，被插入节点为左节点或者右节点。  

- 对维护过程的理解：从 2-3-4 树的角度出发  
《算法导论》中介绍的维护过程并不是一个容易理解的维护过程，因此我们从 2-3-4 树转化为红黑树的角度出发，来探究为何这样操作。  
1. 插入的节点为红色节点，因此其结果就是在 2-3-4 树的某个叶节点上增加了一个关键字。  
2. 规则 2 被破坏，也即插入的节点是 2-3-4 树的根节点，此时该节点被着色为黑色。  
3. 规则 4 被破坏，也即向一个节点插入的节点位于扩展节点的一侧，从而引起一侧的扩展节点达到了 2 个，这里也同样以左侧为例进行讨论。  
  - 第一种情况，右节点是红节点，左节点是红节点，并且插入了一个红节点，也即总数为 4 个节点。此时，需要从节点中间找出一个关键字，移动到上层节点中，并将剩余的节点按左右成为新的左右节点。对于红黑树而言，其操作就是对节点进行着色。  
  - 第二种情况，右节点不是红节点，左节点是红节点，并且插入了一个红节点，插入的红色节点是右节点。此时总数为 3 个节点，并没有超过最大节点，但是由于红色节点都在黑色节点的左侧，因此需要调整。这里不必直接调整完成，而是可以通过对插入节点的上一层节点执行左旋，从而将情况转换为第三种情况。  
  - 第三种情况，右节点不是红节点，左节点是红节点，并且插入了一个红节点，插入的红色节点是左节点。此时总数为 3 个节点，并没有超过最大节点，但是由于红色节点都在红色节点的左测，因此需要调整。这里可以直接对插入节点的上 2 层节点执行右旋，并调整着色，将下降的节点着色为红色，将上升到上 2 层节点位置的节点着色为黑色。  

注意：这里略去了左旋右旋操作对于 2-3-4 树而言，也是一个基本操作（也即维护了性质）。  

### 第14章 数据结构的扩张

略。

## 第四部分 高级设计和分析技术

主要介绍三种重要结束：动态规划、贪心算法、摊还分析。

**动态规划**通常用来解决最优化问题，在这类问题中，我们通过做出一组选择来达到最优解。在做出选择的同时，通常会生成与原问题形式相同的子问题。当多于一个选择子集都生成相同的子问题时，动态规划技术通常就会很有效。动态规划的关键技术就是对每个子问题都保存解，当其重复出现时即可避免重复求解。

**贪心算法**通常最优化问题，我们做出一组选择来达到最优解。贪心算法的思想是每步选择都追求局部最优。贪心算法对许多问题都能求得最优解，而且速度比动态规划方法快得多，但是，我们并不能总能简单地判断出贪心算法是否有效。**拟阵理论**提供了相应的数学基础，可以帮助我们证明一个贪心算法生成最优解。

**摊还分析**用于分析一类特定的算法，这类算法执行一组相似的操作组成的序列。摊还分析并不是通过分别分析每个操作的实际代价的界来分析操作序列的代价的界，而是直接分析序列整体的实际代价的界。这个方法的一个好处是，虽然某些操作的代价可能很高，但其他很多操作的代价可能很低。摊还分析不仅仅是一种分析工具，它还是一种思考算法设计的方式。

### 第15章 动态规划

动态规划，Dynamic Programming。Programming这里指表格法。

动态规划和分治方法类似，都是通过组合子问题的解来求解原问题的。区别是分治方法将问题划分为互不相交的子问题，而动态规划则处理子问题重叠的情况。如果用分治法处理子问题重叠的情况，则会出现大量的重复计算。动态规划将子问题的解存储在表格中，从而每个重复的子问题只会计算一次。

#### 15.1 钢条切割问题

**钢条切割问题**：给定一段长度为$n$的钢条和一个价格表$P=p_i(i\in [1,2,...,n])$，求切割方案，使得销售收益最大。

不妨假设存在某个最优方案，并且将整个切割过程看成是一步步进行的。将每次对整个钢条或更小的钢条的切割过程看成一个问题，则整个切割过程就是由多个子问题组成的。考虑到收益最大时，所有子问题的收益也最大（否则，就会存在更优的切割方案），这种问题的最优解由相关的子问题的最优解组合而成，并且每个子问题可以独立求解，称为**最优子结构**。

一个更为简单的方法是将钢条切割过程看成是从左至右的切割过程（最终切割的结果是没有顺序的，因此调整切割顺序不影响最终的解）。由于左边已经切割过的段不再进行切割，因此只需要考虑右侧的段是否需要切割。

记$r_n$为长度为$n$的钢条切割得到的最大收益，则切割收益为从最左侧切割一段得到的收益和剩下的钢条的最大收益，即$r_n=\max (r_{n-i} + p_i),i\in [1,n]$，其中，当$i$为$n$时，代表不切割。

##### 15.1.1 使用分治法求解

将钢条的切割过程特化为从左到右之后，整个切割过程可以看作从左侧选择一段，然后利用递归函数求解右侧的切割后的最大收益，计算两者之和作为当前切割方式的收益，并求选择哪一段的切割收益最大。

记计算收益的函数`price_rod`在处理长度为$n$的钢条时，被调用的次数为$T(n)$，显然有$T(0)=1$，$T(n)=1+\sum_{i=0}^{n-1}T(i)$，其中$1$表示表示递归调用的最上层调用。

由该递推公式可以得到，$T(n+1)=2T(n)$，也即有$T(n)=2^{n}$。

指数级的复杂度使得运算的规模扩张非常快，但是通过分析很容易看出来，在递归求解的过程中，大部分时间都浪费在对相同子问题的重复求解上。

##### 15.1.2 使用动态规划求解

在求解的过程中，将已经得到的子问题记录下来，后续在计算某个子问题时，先检查该子问题是否已经计算过，这样就可以避免大量的重复计算。

动态规划求解有两种方式，一种为自顶向下，一种为自底向上。前者即在分治求解的过程中，将计算过程中得到的结果存下来。后者即从小问题开始计算，逐步累积后再计算目标问题。

对于自底向上的计算方式，由于求解每个长度时，所有子长度的解均已知，因此总的求解次数为$1+2+3+...+n$，即时间复杂度为$\Theta(n^2)$。

对于自顶向下的计算方式，计算长度为$n$的长度时，会依赖长度为$n-1$、$n-2$、...、$1$的所有长度的解，每个子问题只会求解一次。由于长度为$n$时存在$n$个循环，因此总的次数是一个等差数列，即时间复杂度是$\Theta (n^2)$。

##### 15.1.3 子问题图

子问题图可以用来表示一个动态规划问题中的子问题以及子问题之间的依赖关系。

子问题图是一个有向图。每个顶点唯一地对应一个子问题。若子问题a的最优解需要用到子问题b的最优解，则子问题图中就会有一条从子问题a的顶点到子问题b的顶点的有向边。

子问题图$G=(V,E)$的规模可以帮助确定动态规划算法的运行时间。由于每个子问题只求解一次，因此算法运行时间等于每个子问题求解时间之和。通常情况下，一个子问题的求解时间和子问题图中对应顶点的度成正比，子问题的数目对应子问题图的顶点数目。因此，通常情况下，**动态规划算法的运行时间与顶点和边的数量呈线性关系**。

##### 15.1.4 重构解

通过对每个子问题保存最优收益值，同时保存对应的切割方案，就可以输出最优解。

#### 15.2 矩阵链乘法

给定一个$n$个矩阵的序列（矩阵链），计算其乘积。由于矩阵的乘法满足结合律，因此任何加括号的方法都会得到相同的结果。

称具有如下性质的矩阵乘积链为**完全括号化**的：单一矩阵，或者是两个完全括号化的矩阵乘积链的积，且已外加括号。

对矩阵链加括号的方式会对矩阵乘积运算的复杂度产生巨大影响。将高度为$h$、宽度为$w$的矩阵表示为$[h,w]$，则$[a,b],[b,c]$两个矩阵相乘的运算复杂度为$\Theta (abc)$。以三个矩阵$[10,100],[100,5],[5,50]$为例，如果按先后顺序计算，则共有$10\times 100\times 5 + 10\times 5\times 50=7500$次乘法，如果先计算后两个矩阵乘法，则共有$100\times 5\times 50+10\times 100\times 50=75000$次乘法。后者的运算次数是前者的10倍。

**矩阵乘法问题**：给定$n$个矩阵的链，求完全括号化方案，使得计算乘积所需标量乘法次数最少。

##### 15.2.1 穷举法的复杂度

完全括号化的过程，从计算的角度看，由于每一步的优先级都是确定的，因此最后一步一定是两个完全括号化的部分积相乘，也即括号化过程可以看作是将矩阵链划分为两个任意的部分积后相乘的形式。

以递归公示的形式表达出来为：

$P(n)=\begin{cases}1 &n=1\\\displaystyle\sum_{k=1}^{n-1}P(k)P(n-k) &n\ge 2\end{cases}$

其中，$P(n)$表示$n$个矩阵链可供选择的括号化方案数量。

可以证明，这个递归公式的结果为$\Omega(2^n)$。

##### 15.2.2 动态规划方法

矩阵链乘法可以用动态规划方法进行最优解求解。

#### 15.3 动态规划原理

本节讨论何时使用动态规划。

适合应用动态规划方法求解的最优化问题应该具备两个要素：**最优子结构**和**子问题重叠**。

##### 15.3.1 最优子结构

如前文所述，如果一个问题的最优解包含其子问题的最优解，我们就称此问题具有**最优子结构**性质。

可以用子问题的总数和每个子问题需要考察多少种选择这两个因素的乘积来粗略分析动态规划算法的运行时间。

##### 15.3.2 注意事项

考虑有向图的两个典型问题：

- 无权最短路径
- 无权最长路径（简单路径）

无权最短路径具有最优子结构的性质，但是无权最长路径不具有最优子结构的性质。

对于无权最短路径问题，假设存在一条路径为无权最短路径（记为$uv$），则从这条路径中取一点$p$，得到了两条路径$up,pv$，可以证明，这两条路径同样也是这两条路径之间的无权最短路径（可以直接反证）。因此无权最短路径问题具有最优子结构性质。

对于无权最长路径问题，其并不满足最优子结构性质（甚至子问题合并之后都不一定是一个合法解）。事实上，这是一个NP完全问题，目前还没有一个多项式时间的解法。

无权最长路径问题不具有最优子结构的一个显著特征是其**子问题之间是相关的**。如果某个子问题用到了某个点，则相邻的子问题在使用这个点的时候需要考虑简单路径的要求。

##### 15.3.3 重叠子问题

适用动态规划方法求解的最优化问题应该具备的第二个性质是子问题空间必须足够小，即问题的递归算法回反复地求解相同的子问题，而不是一直生成新的子问题。一般而言，子问题的总数是输入规模的多项式函数为好。

如果递归算法反复求解相同的子问题，我们就称最优化问题具有**重叠子问题**。与之相对的，**适合用分治法**求解的问题通常在递归的每一步都生成全新的子问题。

#### 15.4 最长公共子序列

生物学上，对比两个碱基串A、B的相似度的时候，其中一个方法是寻找一个碱基串C，这个串的所有碱基既在串A上，也在串B上，且在三个串中出现的顺序相同。可以找的串C的长度越长，则认为两个碱基串越相似。

子序列的形式化定义如下：

给定一个序列$X=<x_1,x_2,...,x_m>$，另一个序列$Z=<z_1,z_2,...,z_n>$，当序列$Z$满足如下条件时称为序列$X$的**子序列**：存在一个严格递增的序列$X$的下标序列$<i_1,i_2,...,i_k>$，满足$x_{i_j}=z_j$。

如果一个序列$Z$同时是序列$A$和序列$B$的子序列，则称序列$Z$是序列$A$和序列$B$的**公共子序列**。

最长公共子序列问题：给定两个序列，求两个序列的最长公共子序列。longest-common-subsequence problem, LCS.

##### 15.4.1 穷举法

找出序列$A$的所有子序列，然后验证该序列是否为$B$的子序列，即为最长公共子序列的穷举解法。对于长度为$n$的序列$A$，其子序列的一共有$2^n$个，也因此穷举法的时间复杂度是指数阶，这对于长序列是不实用的。

##### 15.4.2 是否可以使用动态规划方法

###### 15.4.2.1 最优子结构

LCS问题具有最优子结构。

如果定义序列$X$的前$i$个元素构成的新序列为该序列的**前缀**$X_i$。则最优子结构可以描述为，两个序列的LCS包含了两个序列的前缀的LCS。

可以先假设两个序列$X$,$Y$具有一个LCS为$Z$，通过分别讨论三个序列的最后一个元素的相等关系，得到这个最优子结构的证明。讨论如下：

记$X=<x1,x_2,...,x_m>$，$Y=<y_1,y_2,...,y_n>$为两个序列，$Z=<z_1,z_2,...,z_k>$为$X$和$Y$的任意LCS。

- 如果$x_m=y_n$，则$x_m=y_n=z_k$，且$Z_{k-1}$为$X_{m-1}$和$Y_{n-1}$的一个LCS；
- 否则，如果$x_m\ne z_k$，则$Z为$$X_{m-1}$和$Y$的一个LCS；
- 否则（$y_n\ne z_k$），则$Z$为$X$和$Y_{n-1}$的一个LCS；

三种情况的证明都比较显然，不列举。三种情况都符合最优子结构的性质。

###### 15.4.2.2 递归式

记$c(i,j)$为序列前缀$X_i$和$Y_j$的LCS的长度。根据以上讨论，有以下递归式：

$\displaystyle c[i,j]=\begin{cases}0 &i=0或j=0\\c[i-1,j-1]+1 &x_i=y_j\\max\{c[i-1,j],c[i,j-1]\} &x_i\ne y_j\end{cases}$

###### 15.4.2.3 重叠子问题

以$c[2,3]$为例，在以上递归式中，在最多的情况下（实际上就是LCS的长度为0的情况），我们需要计算$c[2,3],c[2,2],c[2,1],c[2,0],c[1,3],...,c[0,3],c[0,2],c[0,1],c[0,0]$总共$(2+1)\times (3+1)=12$个子问题，也即对于$c[i,j]$，最多存在$(i+1)\times (j+1)=\Theta(ij)$个子问题。

因此，LCS问题也是复合重叠子问题要求的。

###### 15.4.2.4 算法实现

参考代码实现。

#### 15.5 最优二叉搜索树

略。

### 第16章 贪心算法

求解最优化问题的算法通常需要经过一系列的步骤，在每个步骤都面临多种选择。对于许多最优化问题，使用动态规划算法来求最优解有些杀鸡用牛刀了，可以使用更简单、更高效的算法。贪心算法就是这样的算法，它在每一步都做出当时看起来最佳的选择。

贪心算法并不保证得到最优解，但对很多问题确实可以求得最优解。

#### 16.1 活动选择问题

假定有一个活动的集合$S=\{a_1,a_2,...,a_n\}$，这些活动使用同一个资源，这个资源统一时间只能供一个活动使用。每个活动的时间区间为$[s_i,f_i)$。如果两个活动的时间区间不重叠，则称两个活动是兼容的。活动选择问题即选出最大的兼容活动集（这里的最大，指的是集合的元素最多）。

##### 16.1.1 最优子结构

首先验证活动选择问题具有最优子结构性质，也即某个活动集的最大兼容活动集一定包含了其子集（子问题空间）的最大兼容活动集。这个问题的最优子结构性质是显然的。

##### 16.1.2 递归式

令$S_{ij}$表示在$a_i$结束之后开始，且在$a_j$开始之前结束的那些活动的集合。用$c[i,j]$表示集合$S_{ij}$的最优解的大小。再假设$a_k$是$S_{ij}$的最优解中的一个，则问题被划分为两个子问题，即求$S_{ik}$,$S_{kj}$的最优解。

有$\displaystyle c[i,j]=\begin{cases}0 &S_{ij}=\empty\\\max(c_{ik}+c_{kj}+1) &S_{ij}\not = \empty,a_k\in S_{ij}\end{cases}$

##### 16.1.3 贪心选择

活动选择问题可以使用动态规划的方法求解，但是其同时也满足贪心选择的要求。假设活动已经按结束时间排序，在选择第一个活动时，需要尽可能的将结束时间提前，也即第一个活动一定是$a_1$，选择了之后剩余的活动必须在活动$a_1$结束之后才能开始。记$S_k$为在$a_k$结束后开始的任务集合，此时$S_k$即为需要求解的子问题。

定理：对于任意非空子问题$S_k$，令$a_m$是其中结束时间最早的活动，则$a_m$在$S_k$的某个最大兼容活动子集中。

证明：记$A_k$是$S_k$的一个最大兼容活动子集，$a_j$是其中结束最早的活动。取集合$A_k'=A_k-\{a_j\}+\{a_m\}$（即把$a_j$替换成$a_m$），由于$A_k$中元素互不相交，而$a_m$的结束时间一定不晚于$a_j$，因此$A_k'$中的元素也互不相交。由于$|A_k'|=|A_k|$，即$A_k'$也是一个最大兼容活动子集。而$a_m\in A_k'$，故得证。

##### 16.1.4 算法实现

比较简单，略。

#### 16.2 贪心算法原理

