# Deep Learning

## 2 线性代数

### 2.1 标量、向量、矩阵和张量

- 标量(scalar)：一个标量就是一个单独的数。
- 向量(vector)：一个向量是一列数。
- 矩阵(matrix)：矩阵是一个二维数组，其中的每一个元素由两个索引所确定。
- 张量(tensor)：一般的，一个数组中的元素分布在若干维坐标的规则网格中，称为张量。

转置是矩阵的重要操作之一。矩阵的转置是以对角线为轴的镜像，这条从左上角到右下角的对角线被称为主对角线。

### 2.2 矩阵和向量相乘

矩阵乘法是矩阵运算中最重要的操作之一。

我们可以通过将两个或多个矩阵并列并方式以书写矩阵乘法，例如：

$$
\bold{C=AB}
$$

具体地，该乘法被定义为：

$$
C_{i,j}=\sum_{k}A_{i,k}B_{k,j}
$$

两个矩阵的标准乘积不是指两个矩阵中对应元素的乘积，不过，那样的矩阵操作确实是存在的，称为**元素对应乘积**（element-wise product）或者Hadamard乘积，记为$\bold{A}\odot \bold{B}$。

矩阵乘积服从分配律：
$$
\bold{A(B+C)=AB+AC}
$$

矩阵乘积也符合结合律：

$$
\bold{A(BC)=(AB)C}
$$

利用矩阵和向量，可以表达下列线性方程组：

$$
\tag{2.11} \bold{Ax=b}
$$

其中，$\bold{A}\in R^{m\times n}$是一个已知矩阵，$\bold{b}\in R^{m}$是一个已知向量，$\bold{x}\in R^{n}$是一个我们要求解的未知向量。向量$\bold{x}$的每一个元素$x_i$都是未知的。上述式子可以重写为：

$$
\bold{A_{1,:}x}=b_{1}\\\bold{A_{2,:}x}=b_{2}\\...\\\bold{A_{m,:}x}=b_{m}
$$

或者写作：

$$
A_{1,1}x_1+A_{1,2}x_2+...+A_{1,n}x_n=b_1\\
A_{2,1}x_1+A_{2,2}x_2+...+A_{2,n}x_n=b_2\\
...\\
A_{m,1}x_1+A_{m,2}x_2+...+A_{m,n}x_n=b_m\\
$$

矩阵向量乘积符号为这种形式的方程提供了更紧凑的表示。

### 单位矩阵和逆矩阵

任意向量和单位矩阵相乘，都不会改变，我们将保持$n$维向量不变的单位矩阵记作$\bold{I}_n$。

单位矩阵的结构很简单：所有沿主对角线的元素都是1，而其它位置的所有元素都是0。

矩阵$\bold{A}$的逆矩阵称作$\bold{A^{-1}}$，满足：

$$
\bold{A^{-1}A=I_{n}}
$$

逆矩阵主要是作为理论工具使用的，并不会在大多数软件应用程序中使用，这是因为逆矩阵在数字计算机中只能表现出有限的精度。有效使用$\bold{b}$的算法通常可以得到更精确的$\bold{x}$。

### 2.4 线性相关和生成子空间

如果逆矩阵存在，则式2.11对于任何向量$\bold{b}$肯定恰好存在一个解$\bold{A^{-1}b}$。对于任意的矩阵$\bold{A}$，则对于向量$\bold{b}$的某些值，有可能不存在解或者存在无数多个解。不存在多于一个解但不是无限多个解的情况，因为假设$\bold{x}$和$\bold{y}$都是方程组的解，则$\bold{z}=\alpha \bold{x} + (1-\alpha )\bold{y}$也是方程组的解。

将矩阵写成列向量的形式，则方程为$[\bold{a_1,a_2,...,a_n}]\bold{x}=\bold{b}$，也即$\bold{a_1}x_1+\bold{a_2}x_2+...+\bold{a_n}x_n=\bold{b}$。因此方程的解的个数，可以看作矩阵的列向量的线性组合能到达向量$\bold{b}$一共有多少种方式。

这种操作称为线性组合。形式上，一组向量的线性组合，是指每个向量乘以对应的标量系数之后的和，即：

$$
\sum_{i}c_i\bold{v}^{(i)}
$$

一组向量的生成子空间是原始向量线性组合后能抵达的点的集合。

确定$\bold{Ax=b}$是否有解，相当于确定向量$\bold{b}$是否在$\bold{A}$列向量的**生成子空间**中。这个特殊的生成子空间被称为$\bold{A}$的**列空间**或者值域。

为了使方程$\bold{Ax=b}$对于任意向量$\bold{b}\in \Reals^m$都存在解，要求$\bold{A}$的列空间构成整个$\Reals^m$。如果$\Reals^m$中的某个点不在$\bold{A}$的列空间中，那么该点对应的$\bold{b}$（点对应的向量）会使该方程没有解。矩阵$\bold{A}$的列空间是整个$\Reals^m$的要求，意味着$\bold{A}$至少有$m$列，即$n\ge m$，也即$\bold{x}$的维数不小于$\bold{b}$。这是一个有解的必要条件，而不是充分条件。因为有些列向量可能是冗余的。

这种冗余称为**线性相关**。如果一组向量中的任意一个向量都不能表示成其它向量的线性组合，那么这组向量称为**线性无关**。如果某个向量的一组向量中某个向量是一组向量中的某些向量的线性组合，那么将这个向量加入这组向量后不会增加这组向量的生成子空间。这意味着，如果一个矩阵的列空间涵盖整个$\Reals^m$，那么该矩阵必须包含至少一组$m$个线性无关的向量。这是式2.11对于每一个向量$\bold{b}$的取值都有解的充分必要条件。值得注意的是，这个条件是说向量集中恰好有$m$个线性无关的列向量，而不是至少$m$个。

考虑矩阵可逆的情况，需要确保方程至多只有一个解（如果可逆，恰好只有一个解）。因此，至多只有$m$个列向量（如果多于$m$个列向量，而整个维度为$n$，则其中一些向量必然线性相关，也即会有不只一个解）。

综上，矩阵可逆，则矩阵必须是一个方阵，并且所有列向量都是线性无关的。一个列向量线性相关的方阵被称为**奇异的**（singular）。

### 2.5 范数

在机器学习中，我们经常使用称为**范数**的函数来衡量向量大小。形式上，$L^p$范数的定义如下：
$$
||\bold{x}||_{p}=(\sum_{i}|x_i|^{p})^{\frac{1}{p}}
$$
其中，$p\in \Reals$，$p\ge 1$。

范数（包括$L^p$范数）是将向量映射到非负实数的函数。直观上说，向量$\bold{x}$的范数衡量从原点到点$x$的距离。更严格的说，范数是满足一下性质的任意函数：

- $f(\bold{x})=0 \implies \bold{x}=\bold{0}$
- $f(\bold{x+y})\le f(\bold{x})+f(\bold{y})$
- $\forall \alpha \in \Reals, f(\alpha \bold{x})=|\alpha|f(\bold{x})$

当$p=2$时，$L^2$范数称为**欧几里得范数**。它表示从原点除法到向量$\bold{x}$确定的点的欧几里得距离。$L^2$范数在机器学习中出现的十分频繁，经常简化表示为$||\bold{x}||$。平方$L^2$范数也经常用来衡量向量的大小，可以简单的通过点积$\bold{x}^{\top}\bold{x}$计算。

平方$L^2$范数在数学和计算上都比$L^2$范数本身更方便。但是，其在原点附近增长得十分缓慢。在某些机器学习应用中，区分恰好是零和元素和非零但值很小的元素是很重要的。在这种情况下，我们转而使用在各个位置上斜率相同，同时保持简单的数学形式的函数：$L^1$范数。

有时候我们会统计向量中非零元素的个数来衡量向量的大小，但是这个值本身不是范数。$L^1$范数经常作为表示非零元素数目的替代函数。

另外一个经常在机器学习中出现的范数是$L^{\infin}$范数，也被称为最大范数。这个范数表示向量中具有最大幅值的元素的绝对值。
$$
||\bold{x}||_{\infin}=\max_{i}|x_{i}|
$$
有时候我们也希望衡量矩阵的大小。在深度学习中，最常见的是使用**Frobenius范数**，即：
$$
||\bold{A}||_{F}=\sqrt{\sum_{i,j}A^{2}_{i,j}}
$$
其类似于向量的$L^2$范数。

### 2.6 特殊类型的矩阵和向量

**对角矩阵**只在主对角线上含有非零元素，其它位置都是零。我们用$diag(\bold{v})$表示对角元素有向量$\bold{v}$中元素给定的一个对角方阵。计算乘法$diag(\bold{v})\bold{x}$，只需要将$\bold{x}$中的每个元素$x_i$放大$v_i$倍，也即$diag(\bold{v})\bold{x}=\bold{v}\odot \bold{x}$。在很多情况下，我们可以根据任意矩阵导出一些通用的机器学习算法，但通过将一些矩阵限制为对角矩阵，可以得到计算代价较低的算法。

并非所有的对角矩阵都是方阵。长方形的矩阵也可能是对角矩阵。

**对称矩阵**是转置和自己相等的矩阵，即
$$
\bold{A}=\bold{A}^\top
$$
**单位向量**是具有单位范数的向量，即
$$
||\bold{x}||_2=1
$$
**正交矩阵**是指行向量和列向量分别标准正交的方阵，即
$$
\bold{A}^\top\bold{A}=\bold{A}\bold{A}^\top=\bold{I}
$$
这也意味着
$$
\bold{A}^{-1}=\bold{A}^\top
$$

### 2.7 特征分解

许多数学对象可以通过将它们分解成多个组成部分或者找到它们的一些属性来更好的理解。这些属性是通用的，而不是由我们选择表示它们的方式所产生的。

例如，整数可以分解为质因数。$12=2\times 2\times 3$，从这个表示中，我们知道$12$不能被$5$整除。或者$12$的倍数可以被$3$整除。

**特征分解**是使用最广的矩阵分解之一，即我们将矩阵分解成一组特征向量和特征值。

方阵$\bold{A}$的**特征向量**是指与$\bold{A}$相乘后相当于对该向量进行缩放的非零向量$\bold{v}$：
$$
\bold{Av}=\lambda \bold{v}
$$
其中标量$\lambda$称为这个特征向量对应的**特征值**。（类似地，我们也可以定义左特征值向量，$\bold{v}^\top\bold{A}=\lambda\bold{v}^{\top}$，但是我们通常更关注右特征值向量）。

如果$\bold{v}$是$\bold{A}$的特征向量，那么任何缩放后的向量$s\bold{v}$（$s\in \Reals, s\ne 0$）也是$\bold{A}$的特征向量，并且$s\bold{v}$和$\bold{v}$具有相同的特征值。因此，我们通常只考虑单位特征向量。

假设矩阵$\bold{A}$有$n$个线性无关的特征向量$\{\bold{v}^{(1)},...,\bold{v}^{(n)}\}$，对应着特征值$\{\lambda_1,...,\lambda_n\}$。我们将特征向量连接成一个矩阵，使得每一列是一个特征向量：$\bold{V}=[\bold{v}^{(1)},...,\bold{v}^{(n)}]$。类似地，我们也可以将特征值连接成一个向量$\bold{\lambda}=[\lambda_1,...,\lambda_n]^\top$。可以得到$\bold{AP}=\bold{P}diag{\bold{\lambda}}$。

因此$\bold{A}$的特征分解可以记作
$$
\bold{A}=\bold{V}diag(\bold{\lambda})\bold{V}^{-1}
$$
从特征向量的定义式可以看出，构建具有特定特征值和特征向量的矩阵，能够在目标方向上延伸空间。

将矩阵分解成特征值和特征向量，可以帮助我们分析矩阵的特定性质，就像质因数分解可以帮忙我们理解整数。不是每一个矩阵都可以分解成特征值和特征向量。本书中，我们只需要分解一类有简单分解的矩阵，具体来讲，每个实矩阵都可以分解为实特征向量和实特征值。

### 2.8 奇异值分解

另一种分解矩阵的方法，称为奇异值分解（singular value decomposition，SVD），是将矩阵分解为奇异向量和奇异值。通过奇异值分解，我们会得到一些与特征分解相同类型的