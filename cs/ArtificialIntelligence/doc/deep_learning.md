# Deep Learning

## 2 线性代数

### 2.1 标量、向量、矩阵和张量

- 标量(scalar)：一个标量就是一个单独的数。
- 向量(vector)：一个向量是一列数。
- 矩阵(matrix)：矩阵是一个二维数组，其中的每一个元素由两个索引所确定。
- 张量(tensor)：一般的，一个数组中的元素分布在若干维坐标的规则网格中，称为张量。

转置是矩阵的重要操作之一。矩阵的转置是以对角线为轴的镜像，这条从左上角到右下角的对角线被称为主对角线。

### 2.2 矩阵和向量相乘

矩阵乘法是矩阵运算中最重要的操作之一。

我们可以通过将两个或多个矩阵并列并方式以书写矩阵乘法，例如：

$$
\bold{C=AB}
$$

具体地，该乘法被定义为：

$$
C_{i,j}=\sum_{k}A_{i,k}B_{k,j}
$$

两个矩阵的标准乘积不是指两个矩阵中对应元素的乘积，不过，那样的矩阵操作确实是存在的，称为**元素对应乘积**（element-wise product）或者Hadamard乘积，记为$\bold{A}\odot \bold{B}$。

矩阵乘积服从分配律：
$$
\bold{A(B+C)=AB+AC}
$$

矩阵乘积也符合结合律：

$$
\bold{A(BC)=(AB)C}
$$

利用矩阵和向量，可以表达下列线性方程组：

$$
\tag{2.11} \bold{Ax=b}
$$

其中，$\bold{A}\in R^{m\times n}$是一个已知矩阵，$\bold{b}\in R^{m}$是一个已知向量，$\bold{x}\in R^{n}$是一个我们要求解的未知向量。向量$\bold{x}$的每一个元素$x_i$都是未知的。上述式子可以重写为：

$$
\bold{A_{1,:}x}=b_{1}\\\bold{A_{2,:}x}=b_{2}\\...\\\bold{A_{m,:}x}=b_{m}
$$

或者写作：

$$
A_{1,1}x_1+A_{1,2}x_2+...+A_{1,n}x_n=b_1\\
A_{2,1}x_1+A_{2,2}x_2+...+A_{2,n}x_n=b_2\\
...\\
A_{m,1}x_1+A_{m,2}x_2+...+A_{m,n}x_n=b_m\\
$$

矩阵向量乘积符号为这种形式的方程提供了更紧凑的表示。

### 单位矩阵和逆矩阵

任意向量和单位矩阵相乘，都不会改变，我们将保持$n$维向量不变的单位矩阵记作$\bold{I}_n$。

单位矩阵的结构很简单：所有沿主对角线的元素都是1，而其它位置的所有元素都是0。

矩阵$\bold{A}$的逆矩阵称作$\bold{A^{-1}}$，满足：

$$
\bold{A^{-1}A=I_{n}}
$$

逆矩阵主要是作为理论工具使用的，并不会在大多数软件应用程序中使用，这是因为逆矩阵在数字计算机中只能表现出有限的精度。有效使用$\bold{b}$的算法通常可以得到更精确的$\bold{x}$。

### 2.4 线性相关和生成子空间

如果逆矩阵存在，则式2.11对于任何向量$\bold{b}$肯定恰好存在一个解$\bold{A^{-1}b}$。对于任意的矩阵$\bold{A}$，则对于向量$\bold{b}$的某些值，有可能不存在解或者存在无数多个解。不存在多于一个解但不是无限多个解的情况，因为假设$\bold{x}$和$\bold{y}$都是方程组的解，则$\bold{z}=\alpha \bold{x} + (1-\alpha )\bold{y}$也是方程组的解。

将矩阵写成列向量的形式，则方程为$[\bold{a_1,a_2,...,a_n}]\bold{x}=\bold{b}$，也即$\bold{a_1}x_1+\bold{a_2}x_2+...+\bold{a_n}x_n=\bold{b}$。因此方程的解的个数，可以看作矩阵的列向量的线性组合能到达向量$\bold{b}$一共有多少种方式。

这种操作称为线性组合。形式上，一组向量的线性组合，是指每个向量乘以对应的标量系数之后的和，即：

$$
\sum_{i}c_i\bold{v}^{(i)}
$$

一组向量的生成子空间是原始向量线性组合后能抵达的点的集合。

确定$\bold{Ax=b}$是否有解，相当于确定向量$\bold{b}$是否在$\bold{A}$列向量的**生成子空间**中。这个特殊的生成子空间被称为$\bold{A}$的**列空间**或者值域。

为了使方程$\bold{Ax=b}$对于任意向量$\bold{b}\in \Reals^m$都存在解，要求$\bold{A}$的列空间构成整个$\Reals^m$。如果$\Reals^m$中的某个点不在$\bold{A}$的列空间中，那么该点对应的$\bold{b}$（点对应的向量）会使该方程没有解。矩阵$\bold{A}$的列空间是整个$\Reals^m$的要求，意味着$\bold{A}$至少有$m$列，即$n\ge m$，也即$\bold{x}$的维数不小于$\bold{b}$。这是一个有解的必要条件，而不是充分条件。因为有些列向量可能是冗余的。

这种冗余称为**线性相关**。如果一组向量中的任意一个向量都不能表示成其它向量的线性组合，那么这组向量称为**线性无关**。如果某个向量的一组向量中某个向量是一组向量中的某些向量的线性组合，那么将这个向量加入这组向量后不会增加这组向量的生成子空间。这意味着，如果一个矩阵的列空间涵盖整个$\Reals^m$，那么该矩阵必须包含至少一组$m$个线性无关的向量。这是式2.11对于每一个向量$\bold{b}$的取值都有解的充分必要条件。值得注意的是，这个条件是说向量集中恰好有$m$个线性无关的列向量，而不是至少$m$个。

考虑矩阵可逆的情况，需要确保方程至多只有一个解（如果可逆，恰好只有一个解）。因此，至多只有$m$个列向量（如果多于$m$个列向量，而整个维度为$n$，则其中一些向量必然线性相关，也即会有不只一个解）。

综上，矩阵可逆，则矩阵必须是一个方阵，并且所有列向量都是线性无关的。一个列向量线性相关的方阵被称为**奇异的**（singular）。

### 2.5 范数

在机器学习中，我们经常使用称为**范数**的函数来衡量向量大小。形式上，$L^p$范数的定义如下：
$$
||\bold{x}||_{p}=(\sum_{i}|x_i|^{p})^{\frac{1}{p}}
$$
其中，$p\in \Reals$，$p\ge 1$。

范数（包括$L^p$范数）是将向量映射到非负实数的函数。直观上说，向量$\bold{x}$的范数衡量从原点到点$x$的距离。更严格的说，范数是满足一下性质的任意函数：

- $f(\bold{x})=0 \implies \bold{x}=\bold{0}$
- $f(\bold{x+y})\le f(\bold{x})+f(\bold{y})$
- $\forall \alpha \in \Reals, f(\alpha \bold{x})=|\alpha|f(\bold{x})$

当$p=2$时，$L^2$范数称为**欧几里得范数**。它表示从原点除法到向量$\bold{x}$确定的点的欧几里得距离。$L^2$范数在机器学习中出现的十分频繁，经常简化表示为$||\bold{x}||$。平方$L^2$范数也经常用来衡量向量的大小，可以简单的通过点积$\bold{x}^{\top}\bold{x}$计算。

平方$L^2$范数在数学和计算上都比$L^2$范数本身更方便。但是，其在原点附近增长得十分缓慢。在某些机器学习应用中，区分恰好是零和元素和非零但值很小的元素是很重要的。在这种情况下，我们转而使用在各个位置上斜率相同，同时保持简单的数学形式的函数：$L^1$范数。

有时候我们会统计向量中非零元素的个数来衡量向量的大小，但是这个值本身不是范数。$L^1$范数经常作为表示非零元素数目的替代函数。

另外一个经常在机器学习中出现的范数是$L^{\infin}$范数，也被称为最大范数。这个范数表示向量中具有最大幅值的元素的绝对值。
$$
||\bold{x}||_{\infin}=\max_{i}|x_{i}|
$$
有时候我们也希望衡量矩阵的大小。在深度学习中，最常见的是使用**Frobenius范数**，即：
$$
||\bold{A}||_{F}=\sqrt{\sum_{i,j}A^{2}_{i,j}}
$$
其类似于向量的$L^2$范数。

### 2.6 特殊类型的矩阵和向量

**对角矩阵**只在主对角线上含有非零元素，其它位置都是零。我们用$diag(\bold{v})$表示对角元素有向量$\bold{v}$中元素给定的一个对角方阵。计算乘法$diag(\bold{v})\bold{x}$，只需要将$\bold{x}$中的每个元素$x_i$放大$v_i$倍，也即$diag(\bold{v})\bold{x}=\bold{v}\odot \bold{x}$。在很多情况下，我们可以根据任意矩阵导出一些通用的机器学习算法，但通过将一些矩阵限制为对角矩阵，可以得到计算代价较低的算法。

并非所有的对角矩阵都是方阵。长方形的矩阵也可能是对角矩阵。

**对称矩阵**是转置和自己相等的矩阵，即
$$
\bold{A}=\bold{A}^\top
$$
**单位向量**是具有单位范数的向量，即
$$
||\bold{x}||_2=1
$$
**正交矩阵**是指行向量和列向量分别标准正交的方阵，即
$$
\bold{A}^\top\bold{A}=\bold{A}\bold{A}^\top=\bold{I}
$$
这也意味着
$$
\bold{A}^{-1}=\bold{A}^\top
$$

### 2.7 特征分解

许多数学对象可以通过将它们分解成多个组成部分或者找到它们的一些属性来更好的理解。这些属性是通用的，而不是由我们选择表示它们的方式所产生的。

例如，整数可以分解为质因数。$12=2\times 2\times 3$，从这个表示中，我们知道$12$不能被$5$整除。或者$12$的倍数可以被$3$整除。

**特征分解**是使用最广的矩阵分解之一，即我们将矩阵分解成一组特征向量和特征值。

方阵$\bold{A}$的**特征向量**是指与$\bold{A}$相乘后相当于对该向量进行缩放的非零向量$\bold{v}$：
$$
\bold{Av}=\lambda \bold{v}
$$
其中标量$\lambda$称为这个特征向量对应的**特征值**。（类似地，我们也可以定义左特征值向量，$\bold{v}^\top\bold{A}=\lambda\bold{v}^{\top}$，但是我们通常更关注右特征值向量）。

如果$\bold{v}$是$\bold{A}$的特征向量，那么任何缩放后的向量$s\bold{v}$（$s\in \Reals, s\ne 0$）也是$\bold{A}$的特征向量，并且$s\bold{v}$和$\bold{v}$具有相同的特征值。因此，我们通常只考虑单位特征向量。

假设矩阵$\bold{A}$有$n$个线性无关的特征向量$\{\bold{v}^{(1)},...,\bold{v}^{(n)}\}$，对应着特征值$\{\lambda_1,...,\lambda_n\}$。我们将特征向量连接成一个矩阵，使得每一列是一个特征向量：$\bold{V}=[\bold{v}^{(1)},...,\bold{v}^{(n)}]$。类似地，我们也可以将特征值连接成一个向量$\bold{\lambda}=[\lambda_1,...,\lambda_n]^\top$。可以得到$\bold{AP}=\bold{P}diag{\bold{\lambda}}$。

因此$\bold{A}$的特征分解可以记作
$$
\bold{A}=\bold{V}diag(\bold{\lambda})\bold{V}^{-1}
$$
从特征向量的定义式可以看出，构建具有特定特征值和特征向量的矩阵，能够在目标方向上延伸空间。

将矩阵分解成特征值和特征向量，可以帮助我们分析矩阵的特定性质，就像质因数分解可以帮忙我们理解整数。不是每一个矩阵都可以分解成特征值和特征向量。本书中，我们只需要分解一类有简单分解的矩阵，具体来讲，每个实矩阵都可以分解为实特征向量和实特征值。

### 2.8 奇异值分解

另一种分解矩阵的方法，称为奇异值分解（singular value decomposition，SVD），是将矩阵分解为奇异向量和奇异值。通过奇异值分解，我们会得到一些与特征分解相同类型的信息。然而，奇异值分解有更广泛的应用。每个实数矩阵都有一个奇异值分解，但不一定都有特征分解。

和特征分解类似，我们将矩阵$\bold{A}$分解成三个矩阵的乘积：
$$
\bold{A}=\bold{UD}\bold{V}^\top
$$

### 2.9 Moore-Penrose伪逆

对于非方矩阵而言，其逆矩阵没有定义。假设在下面的问题中，我们希望通过矩阵$\bold{A}$的左逆$\bold{B}$来求解线性方程：
$$
\bold{Ax}=\bold{y}
$$
等式两边左乘左逆$\bold{B}$后，我们得到
$$
\bold{x}=\bold{By}
$$
取决于问题的形式，我们可能无法设计一个唯一的映射将$\bold{A}$映射到$\bold{B}$。

如果矩阵$\bold{A}$的行数大于列数，那么上述方程可能没有解，反之，可能有多个解。

Moore-Perrose伪逆使我们在这类问题上取得了一定的进展。矩阵$\bold{A}$的伪逆定义为：
$$
\bold{A}^+=\lim_{\alpha \searrow 0}(\bold{A}^\top\bold{A}+\alpha\bold{I})^{-1}\bold{A}^{\top}
$$
计算伪逆的实际算法没有基于这个定义，而是使用下面的公式：
$$
\bold{A}^+=\bold{V}\bold{D}^+\bold{U}^\top
$$
其中，矩阵$\bold{U}$、$\bold{D}$和$\bold{V}$是矩阵$\bold{A}$奇异值分解后得到的矩阵。

### 2.10 迹运算

迹运算返回的是矩阵对角元素的和：
$$
Tr(\bold{A})=\sum_{i}\bold{A}_{i,j}
$$
若不适用求和符号，很多矩阵运算很难描述，而通过矩阵乘法和迹运算符号可以清楚地表示。

### 2.11 行列式

行列式，记作$det(\bold{A})$，是一个将方阵$\bold{A}$映射到实数的函数。行列式等于矩阵特征值的乘积。行列式的绝对值可以用来衡量矩阵参与矩阵乘法后空间扩大或者缩小了多少。如果行列式是$0$，那么空间至少沿着某一维完全收缩了，使其失去了所有的体积；如果行列式是$1$，那么这个转换保持空间体积不变。

## 2.12 实例：主成分分析

主成分分析（principal components analysis, PCA）是一个简单的机器学习算法，可以通过基础的线性代数知识推导。

假设在$\Reals^n$空间中有$m$个点$\{\bold{x}^{(1)},...,\bold{x}^{(m)}\}$，我们希望对这些点进行有损压缩。有损压缩表示我们使用更少的内存，但损失一些精度去存储这些点。我们希望损失的精度尽可能少。

编码这些点的一种方式是用低维表示。对于每个点$\bold{x}^{(i)}\in \Reals^n$，会有一个对应的编码向量$\bold{c^{(i)}}\in \Reals^l$。如果$l$比$n$小，那么我们便使用了更少的内存来存储原来的数据。我们希望找到一个编码函数，根据输入返回编码，$f(\bold{x})=\bold{c}$。我们也希望找到一个解码函数，给定编码重构输入，$\bold{x}\approx g(f(\bold{x}))$。

PCA由我们选择的编码函数而定。具体来讲，为了简化编码器，我们使用矩阵乘法将编码映射回$\Reals^n$，即$g(\bold{c})=\bold{Dc}$，其中$\bold{D\in\Reals^{n\times l}}$是具体定义解码的矩阵。

到目前为止，所描述的问题可能有多个解。因为如果我们按比例地缩小所有点对应的编码向量$\bold{c}_i$，那么只需按比例放大$\bold{D}_{:,i}$，即可保持结果不变。为了使问题有唯一解，我们限制$\bold{D}$中所有列向量都有单位范数（即$||\bold{d}_i||=1$）（这样就规避了等比例放大的问题）。

计算这个解码器的最优编码可能是一个困难的问题。为了使编码问题简单一些，PCA限制$\bold{D}$的列向量彼此正交。

为了将这个基本想法变为我们能够实现的算法，首先我们需要明确如何根据每一个输入$\bold{x}$得到一个最优编码$\bold{c}^*$。一种方法是最小化原始输入向量$\bold{x}$和重构向量$g(\bold{c}^*)$之间的距离。我们使用范数来衡量它们之间的距离。在PCA算法中，我们使用$L^2$范数
$$
\bold{c}^*=\text{arg min}_{\bold{c}}||\bold{x}-g(\bold{c})||_2
$$

注：$\text{arg min}_\bold{c}$表示使函数取得最小值的自变量值。

我们可以用平方$L^2$范数替代$L^2$范数，因为两者在相同的值$\bold{c}$上取得最小值。这是因为$L^2$范数是非负的，并且平方运算在非负值上是单调递增的。
$$
\bold{c}^*=\text{arg min}_\bold{c}||\bold{x}-g(\bold{c})||_2^2
$$
该最小化函数可以简化成
$$
(\bold{x}-g(\bold{c}))^\top(\bold{x}-g(\bold{c}))\\
=\bold{x}^\top\bold{x}-2\bold{x}^\top g(\bold{c})+g(\bold{c})^\top g(\bold{c})
$$

因为第一项不依赖$\bold{c}$，因此可以忽略。代入$g(\bold{c})$的定义，有
$$
\bold{c}^*=\text{arg min}_{\bold{c}}-2\bold{x}^\top \bold{Dc}+\bold{c}^\top\bold{D}^\top\bold{Dc}
$$

根据矩阵$\bold{D}$的正交性和单位范数约束有

$$
\bold{c}^*=\text{arg min}_{\bold{c}}-2\bold{x}^\top \bold{Dc}+\bold{c}^\top \bold{I}_l\bold{c}
$$

我们可以通过向量微积分来求解这个最优化问题。
$$
\nabla_{\bold{c}}(-2\bold{x}^\top\bold{Dc}+\bold{c}^\top\bold{c})=0\\
-2\bold{D}^\top\bold{x}+2\bold{c}=0\\
\bold{c}=\bold{D}^\top\bold{x}
$$
这使得算法很高效：最优编码$\bold{x}$只需要一个矩阵 - 向量乘法操作。为了编码向量，我们使用编码函数
$$
f(\bold{x})=\bold{D}^\top\bold{x}
$$
进一步使用矩阵乘法，我们也可以定义PCA重构操作：
$$

$$



## 5 机器学习基础

深度学习是机器学习的一个特定分支。我们要像充分理解深度学习，必须对机器学习的基本原理有深刻理解。

### 5.1 学习算法

机器学习算法是一种能够从数据中学习的算法。对于“学习”，Mitchell提供了一个简洁的定义：“对于某类任务$T$和性能度量$P$，一个计算机程序被认为可以从经验$E$中学习是指，通过经验$E$改进后，它在任务$T$上由性能度量$P$衡量的性能有所提升”。

#### 5.1.1 任务$T$

机器学习可以让我们解决一些认为设计和使用确定性程序很难解决的问题。从科学和哲学的角度看，机器学习之所以收到关注，是因为提高我们对机器学习的认识需要提高我们自身对智能背后原理的理解。

通常机器学习任务定义为机器学习系统应该如何处理样本。样本是指我们从某些希望机器学习系统处理的对象或事件中收集到的已经量化的特征的集合。我们通常会将样本表示成一个向量$\bold{x}\in \Reals^{n}$。

机器学习可以解决很多类型的任务，一些非常常见的机器学习任务列举如下：

- 分类。在这类任务中，计算机程序需要指定某些输入$k$类中的哪一类。为了完成这个任务，学习算法通常会返回一个函数$f: \Reals \rarr \{1,...,k\}$。当$y=f(\bold{x})$时，模型将向量$\bold{x}$所代表的输入分类到数字码$y$所代表的的类别。
- 输入缺失分类。当输入向量的每个度量不被保证时，分类问题将会变得更具有挑战性。为了解决分类任务，学习算法只需要定义一个从输入向量映射到输出类别的函数。当一些输入可能丢失时，学习算法只需要定义一个从输入向量映射到输出类别的函数。当一些输入可能丢失时，学习算法必须学习一组函数，而不是单个分类函数。每个函数对应着分类具有不同缺失输入子集的$\bold{x}$。这种情况在医疗诊断中经常出现，因为很多类型的医学测试是昂贵的，对身体有害的。
- 回归。在这类任务中，计算机程序需要对给定输入预测数值。为了解决这个任务，学习算法需要输出函数：$f: \Reals^n \rarr \Reals$。
- 转录。在这类任务中，机器学习系统观测一些相对非结构化表示的数据，并转录信息为离散的文本形式。例如，光学字符识别要求计算机程序根据文本图片返回文字序列。另一个例子是语音识别，计算机程序输入一段音频波形，输出一序列音频记录中所说的字符或单词ID的编码。深度学习是现代语音识别系统的重要组成部分。
- 机器翻译。在这类任务中，输入是一种语言的符号序列，计算机程序必须将其转换为另一种语言的符号序列。
- 结构化输出。结构化输出任务的输出是向量或者其他包含多个值的数据结构，并且构成输出的这些不同元素间具有重要关系。这是一个很大的范畴，包括上述转录任务和翻译任务在内的很多其它任务。
- 异常检测。在这类人物中，计算机程序在一组时间或对象中筛选，并标记不正常或非典型的个体。异常检测任务的一个示例是信用卡欺诈检测。
- 合成和采样。在这类任务中，机器学习程序生成一些和训练数据相似的新样本。通过机器学习，合成和采样可能在媒体应用中非常有用，可以避免艺术家大量昂贵或者乏味费时的手动工作。例如，视频游戏可以自动生成大型物体或风景的纹理，而不是让艺术家手动标记每个像素。
- 缺失值填补。在这类任务中，机器学习算法给定一个新样本$\bold{x}\in\Reals^n$，$\bold{x}$中某些元素$x_i$缺失。算法必须填补这些缺失值。
- 去噪。 在这类任务中，机器学习算法的输入是，干净样本$\bold{x}\in\Reals^n$经过未知损坏过程后得到的损坏样本$\tilde{\bold{x}}$预测干净的样本$\bold{x}$，或者更一般地预测条件概率分布$p(\bold{x}|\tilde{\bold{x}})$。
- 密度估计或概率质量函数估计。在密度估计问题中，机器学习算法学习函数$p_{model}:\Reals^n\rarr \Reals$。其中，$p_{model}(\bold{x})$可以解释成样本采样空间的概率密度函数或者概率质量函数。要做好这样的任务，算法需要学习观测到的数据的结构。算法必须知道什么情况下样本举起出现，什么情况下不太可能出现。

#### 5.1.2 性能度量$P$

为了评估机器学习算法的能力我们必须设计其性能的定量度量。通常新跟那个度量$P$是特定于系统执行的任务$T$而言的。

对于诸如分类、缺失输入分类和转录任务，我们通常度量模型的**准确率**。准确率是指该模型输出正确结果的样本比率。我们也可以通过**错误率**得到相同的信息，错误率是指该模型输出错误结果的样本比率。

通常，我们会更加关注机器学习算法在未观测数据上的性能如何，因为这将决定其在实际应用中的性能。因此，我们使用**测试集**（test set）数据来评估系统性能，将其与训练机器学习系统的训练集数据分开。

性能度量的选择看上去简单且客观，但是选择一个与系统理想表现对应的性能度量通常是很难的。

在某些情况下，这是因为很难确定应该度量什么。例如，在执行转录任务时，我们是应该度量系统转录整个序列的准确率，还是应该用一个更细粒度的指标，对序列中正确的部分元素以正面评价？在执行回归任务时，我们应该更多地惩罚频繁犯一些中等错误的系统，还是较少犯错但是犯很大错误的系统？这些设计的选择取决于应用。

还有一些情况，我们知道度量哪些数值，但是度量它们不太现实。这种情况经常出现在密度估计中。很多最好的概率模型只能隐式地表示概率分布。在许多这类模型中，计算空间中特定点的概率是不可行的。在这些情况下，我们必须设计一个仍然对应于设计对象的替代标准，或者设计一个理想标准的良好近似。

#### 5.1.3 经验$E$

根据学习过程中的不同经验，机器学习算法可以大致分为**无监督**算法和**监督**算法。

本书中大部分学习算法可以被理解为在整个**数据集**上获取经验。数据集是指很多样本组成的集合。有时我们也将样本称为**数据点**。

**无监督学习算法**（unsupervised learning algorithm）训练含有很多特征的数据集，然后学习出这个数据集上有用的性质。在深度学习中，我们通常要学习生成数据集的整个概率分布。显式地，比如密度估计，或是隐式地，比如合成或去噪。还有一些其他类型的无监督学习任务，例如聚类，将数据集分成相似样本的集合。

**监督学习算法**（supervised learning algorithm）训练含有很多特征的数据集，不过数据集中的样本都有一个**标签**（label）或**目标**（target）。

大致来说，无监督学习涉及观察随机向量$\bold{x}$的好几个样本，试图显式或隐式地学习出概率分布$p(\bold{x})$，或是该分布的一些有意思的性质；而监督学习包含观察随机向量$\bold{x}$及其相关联的值或向量$\bold{y}$，然后从$\bold{x}$预测$\bold{y}$，通常是估计$p(\bold{y}|\bold{x})$。