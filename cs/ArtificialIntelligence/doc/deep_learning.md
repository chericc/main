# Deep Learning

## 2 线性代数

### 2.1 标量、向量、矩阵和张量

- 标量(scalar)：一个标量就是一个单独的数。
- 向量(vector)：一个向量是一列数。
- 矩阵(matrix)：矩阵是一个二维数组，其中的每一个元素由两个索引所确定。
- 张量(tensor)：一般的，一个数组中的元素分布在若干维坐标的规则网格中，称为张量。

转置是矩阵的重要操作之一。矩阵的转置是以对角线为轴的镜像，这条从左上角到右下角的对角线被称为主对角线。

### 2.2 矩阵和向量相乘

矩阵乘法是矩阵运算中最重要的操作之一。

我们可以通过将两个或多个矩阵并列并方式以书写矩阵乘法，例如：

$$\bold{C=AB}$$

具体地，该乘法被定义为：

$$C_{i,j}=\sum_{k}A_{i,k}B_{k,j}$$

矩阵乘积服从分配律：

$$\bold{A(B+C)=AB+AC}$$

矩阵乘积也符合结合律：

$$\bold{A(BC)=(AB)C}$$

利用矩阵和向量，可以表达下列线性方程组：

$$\bold{Ax=b}$$

其中，$\bold{A}\in R^{m\times n}$是一个已知矩阵，$\bold{b}\in R^{n}$是一个已知向量，$\bold{x}\in R^{n}$是一个我们要求解的未知向量。向量$\bold{x}$的每一个元素$x_i$都是未知的。上述式子可以重写为：

$$\bold{A_{1,:}x}=b_{1}\\\bold{A_{2,:}x}=b_{2}\\...\\\bold{A_{m,:}x}=b_{m}$$

或者写作：

$$
A_{1,1}x_1+A_{1,2}x_2+...+A_{1,n}x_n=b_1\\
A_{2,1}x_1+A_{2,2}x_2+...+A_{2,n}x_n=b_2\\
...\\
A_{m,1}x_1+A_{m,2}x_2+...+A_{m,n}x_n=b_m\\
$$

矩阵向量乘积符号为这种形式的方程提供了更紧凑的表示。

### 单位矩阵和逆矩阵

任意向量和单位矩阵相乘，都不会改变，我们将保持$n$维向量不变的单位矩阵记作$\bold{I}_n$。

单位矩阵的结构很简单：所有沿主对角线的元素都是1，而其它位置的所有元素都是0。

矩阵$\bold{A}$的逆矩阵称作$\bold{A^{-1}}$，满足：

$$\bold{A^{-1}A=I_{n}}$$

逆矩阵主要是作为理论工具使用的，并不会在大多数软件应用程序中使用，这是因为逆矩阵在数字计算机中只能表现出有限的精度。有效使用$\bold{b}$的算法通常可以得到更精确的$\bold{x}$。

### 2.4 线性相关和生成子空间

